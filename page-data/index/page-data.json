{"componentChunkName":"component---src-pages-index-tsx","path":"/","result":{"data":{"allPaperDataJson":{"edges":[{"node":{"abstract":"Long-tailed data is still a big challenge for deep neural networks, even though they have achieved great success on balanced data. We observe that vanilla training on long-tailed data with cross-entropy loss makes the instance-rich head classes severely squeeze the spatial distribution of the tail classes, which leads to difficulty in classifying tail class samples. Furthermore, the original cross-entropy loss can only propagate gradient short-lively because the gradient in softmax form rapidly approaches zero as the logit difference increases. This phenomenon is called softmax saturation. It is unfavorable for training on balanced data, but can be utilized to adjust the validity of the samples in long-tailed data, thereby solving the distorted embedding space of long-tailed problems. To this end, this paper proposes the Gaussian clouded logit adjustment by Gaussian perturbation of different class logits with varied amplitude. We define the amplitude of perturbation as cloud size and set relatively large cloud sizes to tail classes. The large cloud size can reduce the softmax saturation and thereby making tail class samples more active as well as enlarging the embedding space. To alleviate the bias in a classifier, we therefore propose the class-based effective number sampling strategy with classifier re-training. Extensive experiments on benchmark datasets validate the superior performance of the proposed method. Source code is available at: https://github.com/Keke921/GCLLoss.","citations":2,"title":"Long-Tailed Visual Recognition via Gaussian Clouded Logit Adjustment","twitter":null,"id":"4cb7a126-0fda-5538-ab0c-2480b95240f5","s2id":"8e5fa110cdeea486d2d29ec838e21d268af8fc0c","pdf":"/content/CVPR2022/papers/Li_Long-Tailed_Visual_Recognition_via_Gaussian_Clouded_Logit_Adjustment_CVPR_2022_paper.pdf","posterSession":"CVPR","authors":["Mengke Li","Yiu-ming Cheung","Yang Lu"],"arXiv":null}},{"node":{"abstract":"Conventional de-noising methods rely on the assumption that the noisy samples are independent and identically distributed, so the resultant classifier, though disturbed by noise, can still easily identify the noises as outliers. However, the assumption is unrealistic in large-scale data that is inevitably long-tailed. Such imbalance makes a classifier less discriminative for the tail classes, whose previously “easy” noises are now turned into “hard” ones--they are almost as outliers as the tail samples. We introduce this new challenge as Noisy Long-Tailed Classification (NLT). Not surprisingly, we find that most de-noising methods fail to identify the hard noises, resulting in significant performance drop on the three proposed NLT benchmarks: ImageNet-NLT, Animal10-NLT, and Food101-NLT. To this end, we design an iterative noisy learning framework called Hard-to-Easy (H2E). Our bootstrapping philosophy is to first learn a classifier as noise identifier invariant to the class and context distributional change, reducing “hard” noises to “easy” ones, whose removal further improves the invariance. Experimental results show that our H2E outperforms state-of-the-art de-noising methods and their ablations on long-tailed settings while maintaining a stable performance on balanced ones. Codes are in Appendix.","citations":1,"title":"Identifying Hard Noise in Long-Tailed Sample Distribution","twitter":null,"id":"59f6c045-c87f-5bbe-834c-5f84b9cc7239","s2id":"491433941fc8421a624082d95edb17aa6d53ffc4","pdf":"papers/eccv_2022/papers_ECCV/papers/136860725.pdf","posterSession":"ECCV","authors":["Xuanyu Yi","Kaihua Tang","Xian-Sheng Hua","Joo-Hwee Lim","Hanwang Zhang"],"arXiv":null}},{"node":{"abstract":"The problem of long-tailed recognition, where the number of examples per class is highly unbalanced, is considered. While training with class-balanced sampling has been shown effective for this problem, it is known to over-fit to few-shot classes. It is hypothesized that this is due to the repeated sampling of examples and can be addressed by feature space augmentation. A new feature augmentation strategy, EMANATE, based on back-tracking of features across epochs during training, is proposed. It is shown that, unlike class-balanced sampling, this is an adversarial augmentation strategy. A new sampling procedure, Breadcrumb, is then introduced to implement adversarial class-balanced sampling without extra computation. Experiments on three popular long-tailed recognition datasets show that Breadcrumb training produces classifiers that outperform existing solutions to the problem.","citations":1,"title":"Breadcrumbs: Adversarial Class-Balanced Sampling for Long-Tailed Recognition","twitter":null,"id":"7dfcfa9e-81ef-53bd-9335-0166c6a54abd","s2id":"16caa3d5ce814c54ea73c6579bb76ee8a8800294","pdf":"papers/eccv_2022/papers_ECCV/papers/136840628.pdf","posterSession":"ECCV","authors":["Bo Liu","Haoxiang Li","Hao Kang","Gang Hua","Nuno Vasconcelos"],"arXiv":null}},{"node":{"abstract":"Long-tailed image recognition presents massive challenges to deep learning systems since the imbalance between majority (head) classes and minority (tail) classes severely skews the data-driven deep neural networks. Previous methods tackle with data imbalance from the viewpoints of data distribution, feature space, and model design, etc.In this work, instead of directly learning a recognition model, we suggest confronting the bottleneck of head-to-tail bias before classifier learning, from the previously omitted perspective of balancing label space. To alleviate the head-to-tail bias, we propose a concise paradigm by progressively adjusting label space and dividing the head classes and tail classes, dynamically constructing balance from imbalance to facilitate the classification. With flexible data filtering and label space mapping, we can easily embed our approach to most classification models, especially the decoupled training methods. Besides, we find the separability of head-tail classes varies among different features with different inductive biases. Hence, our proposed model also provides a feature evaluation method and paves the way for long-tailed feature learning. Extensive experiments show that our method can boost the performance of state-of-the-arts of different types on widely-used benchmarks. Code is available at https://github.com/silicx/DLSA.","citations":2,"title":"Constructing Balance from Imbalance for Long-Tailed Image Recognition","twitter":{"likes":0,"replies":0,"retweets":0,"ids":["1555451002161172487"]},"id":"351a426a-e760-50fc-abcf-310f6f0bca87","s2id":"dbf40ffd487e94683f9a9d01e4ecbf64a38bdd2e","pdf":"papers/eccv_2022/papers_ECCV/papers/136800036.pdf","posterSession":"ECCV","authors":["Yue Xu","Yong-Lu Li","Jiefeng Li","Cewu Lu"],"arXiv":null}},{"node":{"abstract":"In the real open world, data tends to follow long-tailed class distributions, motivating the well-studied long-tailed recognition (LTR) problem. Naive training produces models that are biased toward common classes in terms of higher accuracy. The key to addressing LTR is to balance various aspects including data distribution, training losses, and gradients in learning. We explore an orthogonal direction,  weight balancing , motivated by the empirical observation that the naively trained classifier has \"artificially\" larger weights in norm for common classes (because there exists abundant data to train them, unlike the rare classes). We investigate three techniques to balance weights, L2-normalization, weight decay, and MaxNorm. We first point out that L2-normalization \"perfectly\" balances per-class weights to be unit norm, but such a hard constraint might prevent classes from learning better classifiers. In contrast, weight decay penalizes larger weights more heavily and so learns small balanced weights; the MaxNorm constraint encourages growing small weights within a norm ball but caps all the weights by the radius. Our extensive study shows that both help learn balanced weights and greatly improve the LTR accuracy. Surprisingly, weight decay, although underexplored in LTR, significantly improves over prior work. Therefore, we adopt a two-stage training paradigm and propose a simple approach to LTR: (1) learning features using the cross-entropy loss by tuning weight decay, and (2) learning classifiers using class-balanced loss by tuning weight decay and MaxNorm. Our approach achieves the state-of-the-art accuracy on five standard benchmarks, serving as a future baseline for long-tailed recognition.","citations":7,"title":"Long-Tailed Recognition via Weight Balancing","twitter":{"likes":84,"replies":2,"retweets":16,"ids":["1508636533884166148","1526487407738036224","1539012116862074881","1548951886744604673","1508639308722290689"]},"id":"3697c677-7631-5eb4-9bec-3e626ff908d7","s2id":"cc8e9f795f83c5107816bd500acb13c4e200198c","pdf":"/content/CVPR2022/papers/Alshammari_Long-Tailed_Recognition_via_Weight_Balancing_CVPR_2022_paper.pdf","posterSession":"CVPR","authors":["Shaden Alshammari","Yu-Xiong Wang","Deva Ramanan","Shu Kong"],"arXiv":"http://arxiv.org/abs/2203.14197"}},{"node":{"abstract":"Existing long-tailed classification (LT) methods only focus on tackling the class-wise imbalance that head classes have more samples than tail classes, but overlook the attribute-wise imbalance. In fact, even if the class is balanced, samples within each class may still be long-tailed due to the varying attributes. Note that the latter is fundamentally more ubiquitous and challenging than the former because attributes are not just implicit for most datasets, but also combinatorially complex, thus prohibitively expensive to be balanced. Therefore, we introduce a novel research problem: Generalized Long-Tailed classification (GLT), to jointly consider both kinds of imbalances. By “generalized”, we mean that a GLT method should naturally solve the traditional LT, but not vice versa. Not surprisingly, we find that most class-wise LT methods degenerate in our proposed two benchmarks: ImageNet-GLT and MSCOCO-GLT. We argue that it is because they over-emphasize the adjustment of class distribution while neglecting to learn attribute-invariant features. To this end, we propose an Invariant Feature Learning (IFL) method as the first strong baseline for GLT. IFL first discovers environments with divergent intra-class distributions from the imperfect predictions, and then learns invariant features across them. Promisingly, as an improved feature backbone, IFL boosts all the LT line-up: one/two-stage re-balance, augmentation, and ensemble. Codes and benchmarks are available on Github: https://github.com/KaihuaTang/Generalized-Long-Tailed-Benchmarks.pytorch","citations":1,"title":"Invariant Feature Learning for Generalized Long-Tailed Classification","twitter":null,"id":"eeb1f7fc-68f8-5713-a45c-1e4f00b2abc9","s2id":"3c74081eb64c2c99f0412ea0c1578dc08e74b715","pdf":"papers/eccv_2022/papers_ECCV/papers/136840698.pdf","posterSession":"ECCV","authors":["Kaihua Tang","Mingyuan Tao","Jiaxin Qi","Zhenguang Liu","Hanwang Zhang"],"arXiv":null}},{"node":{"abstract":"Classification on long-tailed distributed data is a challenging problem, which suffers from serious class-imbalance and accordingly unpromising performance especially on tail classes. Recently, the ensembling based methods achieve the state-of-the-art performance and show great potential. However, there are two limitations for current methods. First, their predictions are not trustworthy for failure-sensitive applications. This is especially harmful for the tail classes where the wrong predictions is basically frequent. Second, they assign unified numbers of experts to all samples, which is redundant for easy samples with excessive computational cost. To address these issues, we propose a Trustworthy Long-tailed Classification (TLC) method to jointly conduct classification and uncertainty estimation to identify hard samples in a multi-expert framework. Our TLC obtains the evidence-based uncertainty (EvU) and evidence for each expert, and then combines these uncertainties and evidences under the Dempster-Shafer Evidence Theory (DST). Moreover, we propose a dynamic expert engagement to reduce the number of engaged experts for easy samples and achieve efficiency while maintaining promising performances. Finally, we conduct comprehensive experiments on the tasks of classification, tail detection, OOD detection and failure prediction. The experimental results show that the proposed TLC outperforms existing methods and is trustworthy with reliable uncertainty.","citations":4,"title":"Trustworthy Long-Tailed Classification","twitter":null,"id":"9a6759f9-18fd-5157-beeb-fdf643f78726","s2id":"6bdd6bc4b4a8c3e5314f2ffcd2e1a08ae6674c05","pdf":"/content/CVPR2022/papers/Li_Trustworthy_Long-Tailed_Classification_CVPR_2022_paper.pdf","posterSession":"CVPR","authors":["Bolian Li","Zongbo Han","Haining Li","Huazhu Fu","Changqing Zhang"],"arXiv":"http://arxiv.org/abs/2111.09030"}},{"node":{"abstract":"Despite the recent success of long-tailed object detection, almost all long-tailed object detectors are developed based on the two-stage paradigm. In practice, one-stage detectors are more prevalent in the industry because they have a simple and fast pipeline that is easy to deploy. However, in the long-tailed scenario, this line of work has not been explored so far. In this paper, we investigate whether one-stage detectors can perform well in this case. We discover the primary obstacle that prevents one-stage detectors from achieving excellent performance is: categories suffer from different degrees of positive-negative imbalance problems under the long-tailed data distribution. The conventional focal loss balances the training process with the same modulating factor for all categories, thus failing to handle the long-tailed problem. To address this issue, we propose the Equalized Focal Loss (EFL) that rebalances the loss contribution of positive and negative samples of different categories independently according to their imbalance degrees. Specifically, EFL adopts a category-relevant modulating factor which can be adjusted dynamically by the training status of different categories. Extensive experiments conducted on the challenging LVIS v1 benchmark demonstrate the effectiveness of our proposed method. With an end-to-end training pipeline, EFL achieves 29.2% in terms of overall AP and obtains significant performance improvements on rare categories, surpassing all existing state-of-the-art methods. The code is available at https://github.com/ModelTC/EOD.","citations":11,"title":"Equalized Focal Loss for Dense Long-Tailed Object Detection","twitter":null,"id":"32225fa5-f856-5094-adad-c1770847dcaa","s2id":"d1d75ac25fd457166360c346cf89005e2531a5fc","pdf":"/content/CVPR2022/papers/Li_Equalized_Focal_Loss_for_Dense_Long-Tailed_Object_Detection_CVPR_2022_paper.pdf","posterSession":"CVPR","authors":["Bo Li","Yongqiang Yao","Jingru Tan","Gang Zhang","Fengwei Yu","Jianwei Lu","Ye Luo"],"arXiv":"http://arxiv.org/abs/2201.02593"}},{"node":{"abstract":"Major advancements have been made in the field of object detection and segmentation recently. However, when it comes to rare categories, the state-of-the-art methods fail to detect them, resulting in a significant performance gap between rare and frequent categories. In this paper, we identify that Sigmoid or Softmax functions used in deep detectors are a major reason for low performance and are suboptimal for long-tailed detection and segmentation. To address this, we develop a Gumbel Optimized Loss (GOL), for long-tailed detection and segmentation. It aligns with the Gumbel distribution of rare classes in imbalanced datasets, considering the fact that most classes in long-tailed detection have low expected probability. The proposed GOL significantly outperforms the best state-of-the-art method by 1.1% on AP, and boosts the overall segmentation by 9.0% and detection by 8.0%, particularly improving detection of rare classes by 20.3%, compared to Mask-RCNN, on LVIS dataset. Code available at: https://github.com/kostas1515/ GOL.","citations":1,"title":"Long-Tailed Instance Segmentation Using Gumbel Optimized Loss","twitter":null,"id":"fdaa185f-eafa-5165-b431-7dffa6c05f92","s2id":"1d2e206b6806a98f80dc8fde011e9562d418b57a","pdf":"papers/eccv_2022/papers_ECCV/papers/136700349.pdf","posterSession":"ECCV","authors":["Konstantinos Panagiotis Alexandridis","Jiankang Deng","Anh Nguyen","Shan Luo"],"arXiv":null}},{"node":{"abstract":"General object detectors are always evaluated on hand-designed datasets, e.g., MS COCO and Pascal VOC, which tend to maintain balanced data distribution over different classes. However, it goes against the practical applications in the real world which suffer from a heavy class imbalance problem, known as the long-tailed object detection. In this paper, we propose a novel method, named Adaptive Hierarchical Representation Learning (AHRL), from a metric learning perspective to address long-tailed object detection. We visualize each learned class representation in the feature space, and observe that some classes, especially under-represented scarce classes, are prone to cluster with analogous ones due to the lack of discriminative representation. Inspired by this, we propose to split the whole feature space into a hierarchical structure and eliminate the problem in a divide-and-conquer way. AHRL contains a two-stage training paradigm. First, we train a normal baseline model and construct the hierarchical structure under the unsupervised clustering method. Then, we design an AHR loss that consists of two optimization objectives. On the one hand, AHR loss retains the hierarchical structure and keeps representation clusters away from each other. On the other hand, AHR loss adopts adaptive margins according to specific class pairs in the same cluster to further optimize locally. We conduct extensive experiments on the challenging LVIS dataset and AHRL outperforms all the existing state-of-the-art(SOTA) methods, with 29.1% segmentation AP and 29.3% box AP on LVIS v0.5 and 27.6% segmentation AP and 28.7% box AP on LVIS v1.0 based on ResNet-101. We hope our simple yet effective approach will serve as a solid baseline to help stimulate future research in long-tailed object detection. Code will be released soon.","citations":0,"title":"Adaptive Hierarchical Representation Learning for Long-Tailed Object Detection","twitter":null,"id":"3285677c-7a07-502e-bf4f-2b4abfb43b6c","s2id":"e67091d2605692eae303ca6c8fc93e8ea28b20eb","pdf":"/content/CVPR2022/papers/Li_Adaptive_Hierarchical_Representation_Learning_for_Long-Tailed_Object_Detection_CVPR_2022_paper.pdf","posterSession":"CVPR","authors":["Banghuai Li"],"arXiv":null}},{"node":{"abstract":"In class incremental learning (CIL) a model must learn new classes in a sequential manner without forgetting old ones. However, conventional CIL methods consider a balanced distribution for each new task, which ignores the prevalence of long-tailed distributions in the real world. In this work we propose two long-tailed CIL scenarios, which we term Ordered and Shuffled LT-CIL. Ordered LT-CIL considers the scenario where we learn from head classes collected with more samples than tail classes which have few. Shuffled LT-CIL, on the other hand, assumes a completely random long-tailed distribution for each task. We systematically evaluate existing methods in both LT-CIL scenarios and demonstrate very different behaviors compared to conventional CIL scenarios. Additionally, we propose a two-stage learning baseline with a learnable weight scaling layer for reducing the bias caused by long-tailed distribution in LT-CIL and which in turn also improves the performance of conventional CIL due to the limited exemplars. Our results demonstrate the superior performance (up to 6.44 points in average incremental accuracy) of our approach on CIFAR-100 and ImageNet-Subset. The code is available at https://github.com/xialeiliu/Long-Tailed-CIL.","citations":0,"title":"Long-Tailed Class Incremental Learning","twitter":{"likes":1,"replies":0,"retweets":0,"ids":["1577114017855840256","1577097624888479745"]},"id":"a888465b-129d-5d8c-b00b-a11463004df7","s2id":"fc893a23b667bbf9ce4a2faa1f9918763039bba6","pdf":"papers/eccv_2022/papers_ECCV/papers/136930486.pdf","posterSession":"ECCV","authors":["Xialei Liu","Yu-Song Hu","Xu-Sheng Cao","Andrew D. Bagdanov","Ke Li","Ming-Ming Cheng"],"arXiv":null}},{"node":{"abstract":"One fundamental challenge in building an instance segmentation model for a large number of classes in complex scenes is the lack of training examples, especially for rare objects. In this paper, we explore the possibility to increase the training examples without laborious data collection and annotation. We find that an abundance of instance segments can potentially be obtained freely from object-centric images, according to two insights: (i) an object-centric image usually contains one salient object in a simple background; (ii) objects from the same class often share similar appearances or similar contrasts to the background. Motivated by these insights, we propose a simple and scalable framework FreeSeg for extracting and leveraging these \"\"free\"\" object foreground segments to facilitate model training in long-tailed instance segmentation. Concretely, we investigate the similarity among object-centric images of the same class to propose candidate segments of foreground instances, followed by a novel ranking of segment quality. The resulting high-quality object segments can then be used to augment the existing long-tailed datasets, e.g., by copying and pasting the segments onto the original training images. Extensive experiments show that FreeSeg yields substantial improvements on top of strong baselines and achieves state-of-the-art accuracy for segmenting rare object categories. Our code is publicly available at https://github.com/czhang0528/FreeSeg.","citations":0,"title":"Learning with Free Object Segments for Long-Tailed Instance Segmentation","twitter":null,"id":"568334f3-65ac-596b-bbf9-9b9d98d558c6","s2id":"b26efb21196fc92cc2ec5370d2cd852aed5be77a","pdf":"papers/eccv_2022/papers_ECCV/papers/136700648.pdf","posterSession":"ECCV","authors":["Cheng Zhang","Tai-Yu Pan","Tianle Chen","Jike Zhong","Wenjin Fu","Wei-Lun Chao"],"arXiv":null}},{"node":{"abstract":"Imbalanced datasets with long-tailed distribution widely exist in practice, posing great challenges for deep networks on how to handle the biased predictions between head (majority, frequent) classes and tail (minority, rare) classes. Feature space of tail classes learned by deep networks is usually under-represented, causing heterogeneous performance among different classes. Existing methods augment tail-class features to compensate tail classes on feature space, but these methods fail to generalize on test phase. To mitigate this problem, we propose a novel Sample-Adaptive Feature Augmentation (SAFA) to augment features for tail classes resulting in ameliorating the classifier performance. SAFA aims to extract diverse and transferable semantic directions from head classes, and adaptively translate tail-class features along extracted semantic directions for augmentation. SAFA leverages a recycling training scheme ensuring augmented features are sample-specific. Contrastive loss ensures the transferable semantic directions are class-irrelevant and mode seeking loss is adopted to produce diverse tail-class features and enlarge the feature space of tail classes. The proposed SAFA as a plug-in is convenient and versatile to be combined with different methods during training phase without additional computational burden at test time. By leveraging SAFA, we obtain outstanding results on CIFAR-LT-10, CIFAR-LT-100, Places-LT, ImageNet-LT, and iNaturalist2018.","citations":null,"title":"SAFA: Sample-Adaptive Feature Augmentation for Long-Tailed Image Classification","twitter":null,"id":"ab7d05c5-e2cb-50c4-be35-7e5d99b11cd1","s2id":"","pdf":"papers/eccv_2022/papers_ECCV/papers/136840578.pdf","posterSession":"ECCV","authors":["Yan Hong","Jianfu Zhang","Zhongyi Sun","Ke Yan"],"arXiv":null}},{"node":{"abstract":"Continued improvements in deep learning architectures have steadily advanced the overall performance of 3D object detectors to levels on par with humans for certain tasks and datasets, where the overall performance is mostly driven by common examples. However, even the best performing models suffer from the most naive mistakes when it comes to rare examples that do not appear frequently in the training data, such as vehicles with irregular geometries. Most studies in the long-tail literature focus on class-imbalanced classification problems with known imbalanced label counts per class, but they are not directly applicable to the intra-class long-tail examples in problems with large intra-class variations such as 3D object detection, where instances with the same class label can have drastically varied properties such as shapes and sizes. Other works propose to mitigate this problem using active learning based on the criteria of uncertainty, difficulty, or diversity. In this study, we identify a new conceptual dimension - rareness - to mine new data for improving the long-tail performance of models. We show that rareness, as opposed to difficulty, is the key to data-centric improvements for 3D detectors, since rareness is the result of a lack in data support while difficulty is related to the fundamental ambiguity in the problem. We propose a general and effective method to identify the rareness of objects based on density estimation in the feature space using flow models, and propose a principled cost-aware formulation for mining rare object tracks, which improves overall model performance, but more importantly - significantly improves the performance for rare objects (by 30.97%).","citations":0,"title":"Improving the Intra-Class Long-Tail in 3D Detection via Rare Example Mining","twitter":null,"id":"521e4260-a953-5e37-91d8-7e27fee294cb","s2id":"e80b2b224b27718649ae4a2cd91213cbde87b35e","pdf":"papers/eccv_2022/papers_ECCV/papers/136700155.pdf","posterSession":"ECCV","authors":["Chiyu Max Jiang","Mahyar Najibi","Charles R. Qi","Yin Zhou","Dragomir Anguelov"],"arXiv":null}},{"node":{"abstract":"Real-world data often exhibits long tail distributions with heavy class imbalance, where the majority classes can dominate the training process and alter the decision boundaries of the minority classes. Recently, researchers have investigated the potential of supervised contrastive learning for long-tailed recognition, and demonstrated that it provides a strong performance gain. In this paper, we show that while supervised contrastive learning can help improve performance, past baselines suffer from poor uniformity brought in by imbalanced data distribution. This poor uniformity manifests in samples from the minority class having poor separability in the feature space. To address this problem, we propose targeted supervised contrastive learning (TSC), which improves the uniformity of the feature distribution on the hypersphere. TSC first generates a set of targets uniformly distributed on a hypersphere. It then makes the features of different classes converge to these distinct and uniformly distributed targets during training. This forces all classes, including minority classes, to maintain a uniform distribution in the feature space, improves class boundaries, and provides better generalization even in the presence of long-tail data. Experiments on multiple datasets show that TSC achieves state-of-the-art performance on long-tailed recognition tasks.","citations":10,"title":"Targeted Supervised Contrastive Learning for Long-Tailed Recognition","twitter":null,"id":"549c4c67-18dd-57f2-84e2-aaa067bb3388","s2id":"2121c6910b5f187fdaecf65981ed76a6a668a559","pdf":"/content/CVPR2022/papers/Li_Targeted_Supervised_Contrastive_Learning_for_Long-Tailed_Recognition_CVPR_2022_paper.pdf","posterSession":"CVPR","authors":["Tianhong Li","Peng Cao","Yuan Yuan","Lijie Fan","Yuzhe Yang","Rogerio S. Feris","Piotr Indyk","Dina Katabi"],"arXiv":"http://arxiv.org/abs/2111.13998"}},{"node":{"abstract":"Deep long-tailed learning aims to train useful deep networks on practical, real-world imbalanced distributions, wherein most labels of the tail classes are associated with a few samples. There has been a large body of work to train discriminative models for visual recognition on long-tailed distribution. In contrast, we aim to train conditional Generative Adversarial Networks, a class of image generation models on long-tailed distributions. We find that similar to recognition, state-of-the-art methods for image generation also suffer from performance degradation on tail classes. The performance degradation is mainly due to class-specific mode collapse for tail classes, which we observe to be correlated with the spectral explosion of the conditioning parameter matrix. We propose a novel group Spectral Regularizer (gSR) that prevents the spectral explosion alleviating mode collapse, which results in diverse and plausible image generation even for tail classes. We find that gSR effectively combines with existing augmentation and regularization techniques, leading to state-of-the-art image generation performance on long-tailed data. Extensive experiments demonstrate the efficacy of our regularizer on long-tailed datasets with different degrees of imbalance. Project Page: https://sites.google.com/view/gsr-eccv22.","citations":0,"title":"Improving GANs for Long-Tailed Data through Group Spectral Regularization","twitter":null,"id":"088ad4e6-b87a-5060-bfcc-cca932a2421a","s2id":"fe5e75b0a515682f27a8af0db84cc8dacb4f5c4a","pdf":"papers/eccv_2022/papers_ECCV/papers/136750423.pdf","posterSession":"ECCV","authors":["Harsh Rangwani","Naman Jaswani","Tejan Karmali","Varun Jampani","R. Venkatesh Babu"],"arXiv":null}},{"node":{"abstract":"In this work, we introduce a novel strategy for long-tail recognition that addresses the tail classes' few-shot problem via training-free knowledge transfer. Our objective is to transfer knowledge acquired from information-rich common classes to semantically similar, and yet data-hungry, rare classes in order to obtain stronger tail class representations. We leverage the fact that class prototypes and learned cosine classifiers provide two different, complementary representations of class cluster centres in feature space, and use an attention mechanism to select and recompose learned classifiers features from common classes to obtain higher quality rare class representations. Our knowledge transfer process is training free, reducing overfitting risks, and can afford continual extension of classifiers to new classes. Experiments show that our approach can achieve significant performance boosts on rare classes while maintaining robust common class performance, outperforming directly comparable state-of-the-art models.","citations":1,"title":"Long-Tail Recognition via Compositional Knowledge Transfer","twitter":null,"id":"229c925f-5d3d-5f13-bd21-8ae36cae6581","s2id":"3e72d6de13ad6ab62318d7d07df76c02b24d280f","pdf":"/content/CVPR2022/papers/Parisot_Long-Tail_Recognition_via_Compositional_Knowledge_Transfer_CVPR_2022_paper.pdf","posterSession":"CVPR","authors":["Sarah Parisot","Pedro M. Esperança","Steven McDonagh","Tamas J. Madarasz","Yongxin Yang","Zhenguo Li"],"arXiv":null}},{"node":{"abstract":"The networks trained on the long-tailed dataset vary remarkably, despite the same training settings, which shows the great uncertainty in long-tailed learning. To alleviate the uncertainty, we propose a Nested Collaborative Learning (NCL), which tackles the problem by collaboratively learning multiple experts together. NCL consists of two core components, namely Nested Individual Learning (NIL) and Nested Balanced Online Distillation (NBOD), which focus on the individual supervised learning for each single expert and the knowledge transferring among multiple experts, respectively. To learn representations more thoroughly, both NIL and NBOD are formulated in a nested way, in which the learning is conducted on not just all categories from a full perspective but some hard categories from a partial perspective. Regarding the learning in the partial perspective, we specifically select the negative categories with high predicted scores as the hard categories by using a proposed Hard Category Mining (HCM). In the NCL, the learning from two perspectives is nested, highly related and complementary, and helps the network to capture not only global and robust features but also meticulous distinguishing ability. Moreover, self-supervision is further utilized for feature enhancement. Extensive experiments manifest the superiority of our method with outperforming the state-of-the-art whether by using a single model or an ensemble.","citations":1,"title":"Nested Collaborative Learning for Long-Tailed Visual Recognition","twitter":{"likes":3,"replies":1,"retweets":1,"ids":["1509155722335981569","1509015755136077824","1509010907610771457"]},"id":"9cae62a5-8a0a-587c-b053-148e61edff1f","s2id":"44bdf13111592afad8861a5ee19ec4cebfb82b6b","pdf":"/content/CVPR2022/papers/Li_Nested_Collaborative_Learning_for_Long-Tailed_Visual_Recognition_CVPR_2022_paper.pdf","posterSession":"CVPR","authors":["Jun Li","Zichang Tan","Jun Wan","Zhen Lei","Guodong Guo"],"arXiv":"http://arxiv.org/abs/2203.15359"}},{"node":{"abstract":"The problem of class imbalanced data is that the generalization performance of the classifier deteriorates due to the lack of data from minority classes. In this paper, we propose a novel minority over-sampling method to augment diversified minority samples by leveraging the rich context of the majority classes as background images. To diversify the minority samples, our key idea is to paste an image from a minority class onto rich-context images from a majority class, using them as background images. Our method is simple and can be easily combined with the existing long-tailed recognition methods. We empirically prove the effectiveness of the proposed oversampling method through extensive experiments and ablation studies. Without any architectural changes or complex algorithms, our method achieves state-of-the-art performance on various long-tailed classification benchmarks. Our code is made available at https://github.com/naver-ai/cmo.","citations":3,"title":"The Majority Can Help the Minority: Context-Rich Minority Oversampling for Long-Tailed Classification","twitter":null,"id":"e6abb322-1b60-5763-b0e3-0d31d2009972","s2id":"b1fe28f40bfe5e74222b5e3ba4f6fcd994313e3b","pdf":"/content/CVPR2022/papers/Park_The_Majority_Can_Help_the_Minority_Context-Rich_Minority_Oversampling_for_CVPR_2022_paper.pdf","posterSession":"CVPR","authors":["Seulki Park","Youngkyu Hong","Byeongho Heo","Sangdoo Yun","Jin Young Choi"],"arXiv":"http://arxiv.org/abs/2112.00412"}},{"node":{"abstract":"We introduce Retrieval Augmented Classification (RAC), a generic approach to augmenting standard image classification pipelines with an explicit retrieval module. RAC consists of a standard base image encoder fused with a parallel retrieval branch that queries a non-parametric external memory of pre-encoded images and associated text snippets. We apply RAC to the problem of long-tail classification and demonstrate a significant improvement over previous state-of-the-art on Places365-LT and iNaturalist-2018 (14.5% and 6.7% respectively), despite using only the training datasets themselves as the external information source. We demonstrate that RAC's retrieval module, without prompting, learns a high level of accuracy on tail classes. This, in turn, frees the base encoder to focus on common classes, and improve its performance thereon. RAC represents an alternative approach to utilizing large, pretrained models without requiring fine-tuning, as well as a first step towards more effectively making use of external memory within common computer vision architectures.","citations":7,"title":"Retrieval Augmented Classification for Long-Tail Visual Recognition","twitter":{"likes":99,"replies":0,"retweets":13,"ids":["1496677091944845314","1497136293251203075","1544993467956445184","1496757663262453761"]},"id":"56df2c8b-8878-5cd8-943a-33cc0b52e5c9","s2id":"15115f67452f3305b69e6886cee98ac466d42cd5","pdf":"/content/CVPR2022/papers/Long_Retrieval_Augmented_Classification_for_Long-Tail_Visual_Recognition_CVPR_2022_paper.pdf","posterSession":"CVPR","authors":["Alexander Long","Wei Yin","Thalaiyasingam Ajanthan","Vu Nguyen","Pulak Purkait","Ravi Garg","Alan Blair","Chunhua Shen","Anton van den Hengel"],"arXiv":"http://arxiv.org/abs/2202.11233"}},{"node":{"abstract":"Long-tailed instance segmentation is a challenging task due to the extreme imbalance of training samples among classes. It causes severe biases of the head classes (with majority samples) against the tailed ones. This renders \"how to appropriately define and alleviate the bias\" one of the most important issues. Prior works mainly use label distribution or mean score information to indicate a coarse-grained bias. In this paper, we explore to excavate the confusion matrix, which carries the fine-grained misclassification details, to relieve the pairwise biases, generalizing the coarse one. To this end, we propose a novel Pairwise Class Balance (PCB) method, built upon a confusion matrix which is updated during training to accumulate the ongoing prediction preferences. PCB generates fightback soft labels for regularization during training. Besides, an iterative learning paradigm is developed to support a progressive and smooth regularization in such debiasing. PCB can be plugged and played to any existing methods as a complement. Experiments results on LVIS demonstrate that our method achieves state-of-the-art performance without bells and whistles. Superior results across various architectures show the generalization ability. The code and trained models are available at https://github.com/megvii-research/PCB.","citations":3,"title":"Relieving Long-Tailed Instance Segmentation via Pairwise Class Balance","twitter":{"likes":2,"replies":2,"retweets":0,"ids":["1481069354888613890","1511175678418616324","1480812697205841927","1480755219604537346"]},"id":"85c8c7fb-95ee-5e54-862c-cdcaa0a88f3a","s2id":"96187ebced993aa046d93bb99418a154cfbce38a","pdf":"/content/CVPR2022/papers/He_Relieving_Long-Tailed_Instance_Segmentation_via_Pairwise_Class_Balance_CVPR_2022_paper.pdf","posterSession":"CVPR","authors":["Yin-Yin He","Peizhen Zhang","Xiu-Shen Wei","Xiangyu Zhang","Jian Sun"],"arXiv":"http://arxiv.org/abs/2201.02784"}},{"node":{"abstract":"Real-world data often exhibit imbalanced label distributions. Existing studies on data imbalance focus on single-domain settings, i.e., samples are from the same data distribution. However, natural data can originate from distinct domains, where a minority class in one domain could have abundant instances from other domains. We formalize the task of Multi-Domain Long-Tailed Recognition (MDLT), which learns from multi-domain imbalanced data, addresses label imbalance, domain shift, and divergent label distributions across domains, and generalizes to all domain-class pairs. We first develop the domain-class transferability graph, and show that such transferability governs the success of learning in MDLT. We then propose BoDA, a theoretically grounded learning strategy that tracks the upper bound of transferability statistics, and ensures balanced alignment and calibration across imbalanced domain-class distributions. We curate five MDLT benchmarks based on widely-used multi-domain datasets, and compare BoDA to twenty algorithms that span different learning strategies. Extensive and rigorous experiments verify the superior performance of BoDA. Further, as a byproduct, BoDA establishes new state-of-the-art on Domain Generalization benchmarks, highlighting the importance of addressing data imbalance across domains, which can be crucial for improving generalization to unseen domains. Code and data are available at: https://github.com/YyzHarry/multi-domain-imbalance.","citations":1,"title":"On Multi-Domain Long-Tailed Recognition, Imbalanced Domain Generalization and Beyond","twitter":{"likes":69,"replies":0,"retweets":17,"ids":["1546765420203491329","1551283326866149376","1551474050890977281","1551473394155245568","1551283538187673605"]},"id":"179a5792-08dc-57aa-936f-e2e052054c40","s2id":"1272ca88a705b0be51124120e6959a3f7ef5bfec","pdf":"papers/eccv_2022/papers_ECCV/papers/136800054.pdf","posterSession":"ECCV","authors":["Yuzhe Yang","Hao Wang","Dina Katabi"],"arXiv":null}},{"node":{"abstract":"Machine learning models fail to perform well on real-world applications when 1) the category distribution P(Y) of the training dataset suffers from long-tailed distribution and 2) the test data is drawn from different conditional distributions P(X|Y). Existing approaches cannot handle the scenario where both issues exist, which however is common for real-world applications. In this study, we took a step forward and looked into the problem of long-tailed classification under domain shifts. We designed three novel core functional blocks including Distribution Calibrated Classification Loss, Visual-Semantic Mapping and Semantic-Similarity Guided Augmentation. Furthermore, we adopted a meta-learning framework which integrates these three blocks to improve domain generalization on unseen target domains. Two new datasets were proposed for this problem, named AWA2-LTS and ImageNet-LTS. We evaluated our method on the two datasets and extensive experimental results demonstrate that our proposed method can achieve superior performance over state-of-the-art long-tailed/domain generalization approaches and the combinations. Source codes and datasets can be found at our project page https://xiaogu.site/LTDS.","citations":1,"title":"Tackling Long-Tailed Category Distribution under Domain Shifts","twitter":null,"id":"d2c3bf99-45ae-5292-98e8-a7616d3d029e","s2id":"fa5fec041dfcb83c5c357d24743d03de59fbab26","pdf":"papers/eccv_2022/papers_ECCV/papers/136830706.pdf","posterSession":"ECCV","authors":["Xiao Gu","Yao Guo","Zeju Li","Jianing Qiu","Qi Dou","Yuxuan Liu","Benny Lo","Guang-Zhong Yang"],"arXiv":null}},{"node":{"abstract":"The visual relationship recognition (VRR) task aims at understanding the pairwise visual relationships between interacting objects in an image. These relationships typically have a long-tail distribution due to their compositional nature. This problem gets more severe when the vocabulary becomes large, rendering this task very challenging. This paper shows that modeling an effective message-passing flow through an attention mechanism can be critical to tackling the compositionality and long-tail challenges in VRR. The method, called RelTransformer, represents each im- age as a fully-connected scene graph and restructures the whole scene into the relation-triplet and global-scene contexts. It directly passes the message from each element in the relation-triplet and global-scene contexts to the target relation via self-attention. We also design a learnable memory to augment the long-tail relation representation learning. Through extensive experiments, we find that our model generalizes well on many VRR benchmarks. Our model outperforms the best-performing models on two large-scale long-tail VRR benchmarks, VG8K-LT (+2.0% overall acc) and GQA-LT (+26.0% overall acc), both having a highly skewed distribution towards the tail. It also achieves strong results on the VG200 relation detection task. Our code is available at https://github.com/Vision-CAIR/ RelTransformer.","citations":0,"title":"RelTransformer: A Transformer-Based Long-Tail Visual Relationship Recognition","twitter":{"likes":4,"replies":1,"retweets":1,"ids":["1504418916440944641","1509016360214433801","1386852861141602307"]},"id":"e4e0bbc1-8ad4-5eb0-8414-3cac72feb57b","s2id":"20e45053c133eff939f800821230ff589bbdc626","pdf":"/content/CVPR2022/papers/Chen_RelTransformer_A_Transformer-Based_Long-Tail_Visual_Relationship_Recognition_CVPR_2022_paper.pdf","posterSession":"CVPR","authors":["Jun Chen","Aniket Agarwal","Sherif Abdelkarim","Deyao Zhu","Mohamed Elhoseiny"],"arXiv":"http://arxiv.org/abs/2104.11934"}},{"node":{"abstract":"Long-tailed learning aims to tackle the crucial challenge that head classes dominate the training procedure under severe class imbalance in real-world scenarios. However, little attention has been given to how to quantify the dominance severity of head classes in the representation space. Motivated by this, we generalize the cosine-based classifiers to a von Mises-Fisher (vMF) mixture model, denoted as vMF classifier, which enables to quantitatively measure representation quality upon the hyper-sphere space via calculating distribution overlap coefficient. To our knowledge, this is the first work to measure the representation quality of classifiers and features from the perspective of the distribution overlap coefficient. On top of it, we formulate the inter-class discrepancy and class feature consistency loss terms to alleviate the interference among the classifier weights and align features with classifier weights. Furthermore, a novel post-training calibration algorithm is devised to zero-costly boost the performance via inter-class overlap coefficients. Our models outperform previous work with a large margin and achieve state-of-the-art performance on long-tailed image classification, semantic segmentation, and instance segmentation tasks (e.g., we achieve 55.0% overall accuracy with ResNetXt-50 in ImageNet-LT). Our code is available at https://github.com/VipaiLab/vMF_OP.","citations":0,"title":"Towards Calibrated Hyper-Sphere Representation via Distribution Overlap Coefficient for Long-Tailed Learning","twitter":null,"id":"122817a7-7caf-5bd5-b253-980b237491b0","s2id":"d57a90a2abafc55fcdf7dda2a84ac2fc05aa4d9a","pdf":"papers/eccv_2022/papers_ECCV/papers/136840176.pdf","posterSession":"ECCV","authors":["Hualiang Wang","Siming Fu","Xiaoxuan He","Hangxiang Fang","Zuozhu Liu","Haoji Hu"],"arXiv":null}},{"node":{"abstract":"Recently, computer vision foundation models such as CLIP and ALI-GN, have shown impressive generalization capabilities on various downstream tasks. But their abilities to deal with the long-tailed data still remain to be proved. In this work, we present a novel framework based on pre-trained visual-linguistic models for long-tailed recognition (LTR), termed VL-LTR, and conduct empirical studies on the benefits of introducing text modality for long-tailed recognition tasks. Compared to existing approaches, the proposed VL-LTR has the following merits. (1) Our method can not only learn visual representation from images but also learn corresponding linguistic representation from noisy class-level text descriptions collected from the Internet; (2) Our method can effectively use the learned visual-linguistic representation to improve the visual recognition performance, especially for classes with fewer image samples. We also conduct extensive experiments and set the new state-of-the-art performance on widely-used LTR benchmarks. Notably, our method achieves 77.2\\% overall accuracy on ImageNet-LT, which significantly outperforms the previous best method by over 17 points, and is close to the prevailing performance training on the full ImageNet. Code shall be released.","citations":3,"title":"VL-LTR: Learning Class-Wise Visual-Linguistic Representation for Long-Tailed Visual Recognition","twitter":null,"id":"d5d6cb18-9a8a-5fd4-ae92-e34e0e4c5307","s2id":"0665104edcc9adc0d302e2e7f9f63f32f2390c92","pdf":"papers/eccv_2022/papers_ECCV/papers/136850072.pdf","posterSession":"ECCV","authors":["Changyao Tian","Wenhai Wang","Xizhou Zhu","Jifeng Dai","Yu Qiao"],"arXiv":null}},{"node":{"abstract":"Large-scale object detection and instance segmentation faces a severe data imbalance. The finer-grained object classes become, the less frequent they appear in our datasets. However at test-time, we expect a detector that performs well for all classes and not just the most frequent ones. In this paper, we provide a theoretical understanding of the long-trail detection problem. We show how the commonly used mean average precision evaluation metric on an unknown test-set is bound by a margin-based binary classification error on a long-tailed object-detection training set. We optimize margin-based binary classification error with a novel surrogate objective called Effective Class-Margin Loss (ECM). The ECM loss is simple, theoretically well-motivated, and outperforms other heuristic counterparts on LVIS v1 benchmark over a wide range of architecture and detectors. Code is available at https://github.com/janghyuncho/ECM-Loss.","citations":0,"title":"Long-Tail Detection with Effective Class-Margins","twitter":null,"id":"f77e884f-fd39-58b0-862f-35644cffecb4","s2id":"5ab6f1dec780d7f55b9bd18acf6a437d28ac99bd","pdf":"papers/eccv_2022/papers_ECCV/papers/136680684.pdf","posterSession":"ECCV","authors":["Jang Hyun Cho","Philipp Krähenbühl"],"arXiv":null}},{"node":{"abstract":"Long-tail object detection suffers from poor performance on tail categories. We reveal that the real culprit lies in the extremely imbalanced distribution of the classifier's weight norm. For conventional softmax cross-entropy loss, such imbalanced weight norm distribution yields ill conditioned decision boundary for categories which have small weight norms. To get rid of this situation, we choose to maximize the cosine similarity between the learned feature and the weight vector of target category rather than the inner-product of them. The decision boundary between any two categories is the angular bisector of their weight vectors. Whereas, the absolutely equal decision boundary is suboptimal because it reduces the model's sensitivity to various categories. Intuitively, categories with rich data diversity should occupy a larger area in the classification space while categories with limited data diversity should occupy a slightly small space. Hence, we devise a Category-Aware Angular Margin Loss (C2AM Loss) to introduce an adaptive angular margin between any two categories. Specifically, the margin between two categories is proportional to the ratio of their classifiers' weight norms. As a result, the decision boundary is slightly pushed towards the category which has a smaller weight norm. We conduct comprehensive experiments on LVIS dataset. C2AM Loss brings 4.9 5.2 AP improvements on different detectors and backbones compared with baseline.","citations":1,"title":"C2AM Loss: Chasing a Better Decision Boundary for Long-Tail Object Detection","twitter":null,"id":"b3c4608f-0006-5f3e-aea9-c338b59813f4","s2id":"6b593ea4284000631673cda863ec9cc24d5cbab5","pdf":"/content/CVPR2022/papers/Wang_C2AM_Loss_Chasing_a_Better_Decision_Boundary_for_Long-Tail_Object_CVPR_2022_paper.pdf","posterSession":"CVPR","authors":["Tong Wang","Yousong Zhu","Yingying Chen","Chaoyang Zhao","Bin Yu","Jinqiao Wang","Ming Tang"],"arXiv":null}},{"node":{"abstract":"Real-world data typically follow a long-tailed distribution, where a few majority categories occupy most of the data while most minority categories contain a limited number of samples. Classification models minimizing cross-entropy struggle to represent and classify the tail classes. Although the problem of learning unbiased classifiers has been well studied, methods for representing imbalanced data are under-explored. In this paper, we focus on representation learning for imbalanced data. Recently, supervised contrastive learning has shown promising performance on balanced data recently. However, through our theoretical analysis, we find that for long-tailed data, it fails to form a regular simplex which is an ideal geometric configuration for representation learning. To correct the optimization behavior of SCL and further improve the performance of long-tailed visual recognition, we propose a novel loss for balanced contrastive learning (BCL). Compared with SCL, we have two improvements in BCL: class-averaging, which balances the gradient contribution of negative classes; class-complement, which allows all classes to appear in every mini-batch. The proposed balanced contrastive learning (BCL) method satisfies the condition of forming a regular simplex and assists the optimization of cross-entropy. Equipped with BCL, the proposed two-branch framework can obtain a stronger feature representation and achieve competitive performance on long-tailed benchmark datasets such as CIFAR-10-LT, CIFAR-100-LT, ImageNet-LT, and iNaturalist2018.","citations":4,"title":"Balanced Contrastive Learning for Long-Tailed Visual Recognition","twitter":null,"id":"d2be32b0-9cb2-51dc-98ba-f8585c814836","s2id":"cda7a9f058fde65956252831f2c6dd2a8dc370e8","pdf":"/content/CVPR2022/papers/Zhu_Balanced_Contrastive_Learning_for_Long-Tailed_Visual_Recognition_CVPR_2022_paper.pdf","posterSession":"CVPR","authors":["Jianggang Zhu","Zheng Wang","Jingjing Chen","Yi-Ping Phoebe Chen","Yu-Gang Jiang"],"arXiv":null}}]}},"pageContext":{}},"staticQueryHashes":[]}