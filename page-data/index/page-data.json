{"componentChunkName":"component---src-pages-index-tsx","path":"/","result":{"data":{"allPaperDataJson":{"edges":[{"node":{"abstract":"Several approaches have been proposed in recent literature to alleviate the long-tail problem, mainly in object classification tasks. In this paper, we make the first large-scale study concerning the task of Long-Tail Visual Relationship Recognition (LTVRR). LTVRR aims at improving the learning of structured visual relationships that come from the long-tail (e.g.,\"rabbit grazing on grass\"). In this setup, the subject, relation, and object classes each follow a long-tail distribution. To begin our study and make a future benchmark for the community, we introduce two LTVRR-related benchmarks, dubbed VG8K-LT and GQA-LT, built upon the widely used Visual Genome and GQA datasets. We use these benchmarks to study the performance of several state-of-the-art long-tail models on the LTVRR setup. Lastly, we propose a visiolinguistic hubless (VilHub) loss and a Mixup augmentation technique adapted to LTVRR setup, dubbed as RelMix. Both VilHub and RelMix can be easily integrated on top of existing models and despite being simple, our results show that they can remarkably improve the performance, especially on tail classes. Benchmarks, code, and models have been made available at: https://github.com/Vision-CAIR/LTVRR.","citations":0,"title":"Exploring Long Tail Visual Relationship Recognition With Large Vocabulary","twitter":{"likes":12,"replies":1,"retweets":2,"ids":["1437889403410583557","1442748043254812672","1435137970126864387","1245685541368143874"]},"id":"5eee6031-972f-5b73-a217-5ca3b50d7bf2","s2id":"6407e8ab14f73b961798f2487aa53dc0a49773ec","pdf":"https://openaccess.thecvf.com/content/ICCV2021/papers/Abdelkarim_Exploring_Long_Tail_Visual_Relationship_Recognition_With_Large_Vocabulary_ICCV_2021_paper.pdf","posterSession":"ICCV","authors":["Sherif Abdelkarim","Aniket Agarwal","Panos Achlioptas","Jun Chen","Jiaji Huang","Boyang Li","Kenneth Church","Mohamed Elhoseiny"],"arXiv":"http://arxiv.org/abs/2004.00436"}},{"node":{"abstract":"One-stage long-tailed recognition methods improve the overall performance in a \"seesaw\" manner, i.e., either sacrifice the head's accuracy for better tail classification or elevate the head's accuracy even higher but ignore the tail. Existing algorithms bypass such trade-off by a multi-stage training process: pre-training on imbalanced set and fine-tuning on balanced set. Though achieving promising performance, not only are they sensitive to the generalizability of the pre-trained model, but also not easily integrated into other computer vision tasks like detection and segmentation, where pre-training of classifier solely is not applicable. In this paper, we propose a one-stage long-tailed recognition scheme, ally complementary experts (ACE), where the expert is the most knowledgeable specialist in a sub-set that dominates its training, and is complementary to other experts in the less-seen categories without disturbed by what it has never seen. We design a distribution-adaptive optimizer to adjust the learning pace of each expert to avoid over-fitting. Without special bells and whistles, the vanilla ACE outperforms the current one-stage SOTA method by 3 10% on CIFAR10-LT, CIFAR100-LT, ImageNet-LT and iNaturalist datasets. It is also shown to be the first one to break the \"seesaw\" trade-off by improving the accuracy of the majority and minority categories simultaneously in only one stage.","citations":1,"title":"ACE: Ally Complementary Experts for Solving Long-Tailed Recognition in One-Shot","twitter":{"likes":26,"replies":0,"retweets":5,"ids":["1423448425203474439","1423540146960486406"]},"id":"a056da6a-a66d-5638-881a-29eed879c99a","s2id":"6d592bef8e5a960a8e68c04e009ea6fe5506cf10","pdf":"https://openaccess.thecvf.com/content/ICCV2021/papers/Cai_ACE_Ally_Complementary_Experts_for_Solving_Long-Tailed_Recognition_in_One-Shot_ICCV_2021_paper.pdf","posterSession":"ICCV","authors":["Jiarui Cai","Yizhou Wang","Jenq-Neng Hwang"],"arXiv":"http://arxiv.org/abs/2108.02385"}},{"node":{"abstract":"The availability of large-scale image captioning and visual question answering datasets has contributed significantly to recent successes in vision-and-language pre-training. However, these datasets are often collected with overrestrictive requirements inherited from their original target tasks (e.g., image caption generation), which limit the resulting dataset scale and diversity. We take a step further in pushing the limits of vision-and-language pre-training data by relaxing the data collection pipeline used in Conceptual Captions 3M (CC3M) [Sharma et al. 2018] and introduce the Conceptual 12M (CC12M), a dataset with 12 million image-text pairs specifically meant to be used for vision-and-language pre-training. We perform an analysis of this dataset and benchmark its effectiveness against CC3M on multiple downstream tasks with an emphasis on long-tail visual recognition. Our results clearly illustrate the benefit of scaling up pre-training data for vision-and-language tasks, as indicated by the new state-of-the-art results on both the nocaps and Conceptual Captions benchmarks.","citations":9,"title":"Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts","twitter":{"likes":46,"replies":1,"retweets":12,"ids":["1362596321400672259","1363222877219209223"]},"id":"e1ff4655-4d99-50d9-9140-eaa9617d6997","s2id":"394be105b87e9bfe72c20efe6338de10604e1a11","pdf":"https://openaccess.thecvf.com/content/CVPR2021/papers/Changpinyo_Conceptual_12M_Pushing_Web-Scale_Image-Text_Pre-Training_To_Recognize_Long-Tail_Visual_CVPR_2021_paper.pdf","posterSession":"CVPR","authors":["Soravit Changpinyo","Piyush Sharma","Nan Ding","Radu Soricut"],"arXiv":"http://arxiv.org/abs/2102.08981"}},{"node":{"abstract":"In this paper, we propose a progressive margin loss (PML) approach for unconstrained facial age classification. Conventional methods make strong assumption on that each class owns adequate instances to outline its data distribution, likely leading to bias prediction where the training samples are sparse across age classes. Instead, our PML aims to adaptively refine the age label pattern by enforcing a couple of margins, which fully takes in the in-between discrepancy of the intra-class variance, inter-class variance and class-center. Our PML typically incorporates with the ordinal margin and the variational margin, simultaneously plugging in the globally-tuned deep neural network paradigm. More specifically, the ordinal margin learns to exploit the correlated relationship of the real-world age labels. Accordingly, the variational margin is leveraged to minimize the influence of head classes that misleads the prediction of tailed samples. Moreover, our optimization carefully seeks a series of indicator curricula to achieve robust and efficient model training. Extensive experimental results on three face aging datasets demonstrate that our PML achieves compelling performance compared to state of the arts. Code will be made publicly.","citations":5,"title":"PML: Progressive Margin Loss for Long-Tailed Age Classification","twitter":null,"id":"594fb94d-0698-5f85-828e-a4256a00fb6d","s2id":"ab191ff5ae5d92f2156fbe1e5ae453680dfd6ef6","pdf":"https://openaccess.thecvf.com/content/CVPR2021/papers/Deng_PML_Progressive_Margin_Loss_for_Long-Tailed_Age_Classification_CVPR_2021_paper.pdf","posterSession":"CVPR","authors":["Zongyong Deng","Hao Liu","Yaoxing Wang","Chenyang Wang","Zekuan Yu","Xuehong Sun"],"arXiv":"http://arxiv.org/abs/2103.02140"}},{"node":{"abstract":"The conventional detectors tend to make imbalanced classification and suffer performance drop, when the distribution of the training data is severely skewed. In this paper, we propose to use the mean classification score to indicate the classification accuracy for each category during training. Based on this indicator, we balance the classification via an Equilibrium Loss (EBL) and a Memory-augmented Feature Sampling (MFS) method. Specifically, EBL increases the intensity of the adjustment of the decision boundary for the weak classes by a designed score-guided loss margin between any two classes. On the other hand, MFS improves the frequency and accuracy of the adjustments of the decision boundary for the weak classes through over-sampling the instance features of those classes. Therefore, EBL and MFS work collaboratively for finding the classification equilibrium in long-tailed detection, and dramatically improve the performance of tail classes while maintaining or even improving the performance of head classes. We conduct experiments on LVIS using Mask R-CNN with various backbones including ResNet-50-FPN and ResNet-101-FPN to show the superiority of the proposed method. It improves the detection performance of tail classes by 15.6 AP, and outperforms the most recent long-tailed object detectors by more than 1 AP. Code is available at https://github.com/fcjian/LOCE.","citations":2,"title":"Exploring Classification Equilibrium in Long-Tailed Object Detection","twitter":{"likes":0,"replies":0,"retweets":0,"ids":["1427888891391008773"]},"id":"95ca43de-bd5b-5864-8dbf-60bce6d43e1f","s2id":"0c60551eb21914df8bde071a5e8b28aafab42343","pdf":"https://openaccess.thecvf.com/content/ICCV2021/papers/Feng_Exploring_Classification_Equilibrium_in_Long-Tailed_Object_Detection_ICCV_2021_paper.pdf","posterSession":"ICCV","authors":["Chengjian Feng","Yujie Zhong","Weilin Huang"],"arXiv":"http://arxiv.org/abs/2108.07507"}},{"node":{"abstract":"Long-tailed data distribution is common in many multi-label visual recognition tasks and the direct use of these data for training usually leads to relatively low performance on tail classes. While re-balanced data sampling can improve the performance on tail classes, it may also hurt the performance on head classes in training due to label co-occurrence. In this paper, we propose a new approach to train on both uniform and re-balanced samplings in a collaborative way, resulting in performance improvement on both head and tail classes. More specifically, we design a visual recognition network with two branches: one takes the uniform sampling as input while the other takes the re-balanced sampling as the input. For each branch, we conduct visual recognition using a binary-cross-entropy-based classification loss with learnable logit compensation. We further define a new cross-branch loss to enforce the consistency when the same input image goes through the two branches. We conduct extensive experiments on VOC-LT and COCO-LT datasets. The results show that the proposed method significantly outperforms previous state-of-the-art methods on long-tailed multi-label visual recognition.","citations":2,"title":"Long-Tailed Multi-Label Visual Recognition by Collaborative Training on Uniform and Re-Balanced Samplings","twitter":null,"id":"8d7f850b-d68c-553f-b706-fb01e323eb2f","s2id":"15df058b93ea028d21a9ffe8d78e5f0da826d4c9","pdf":"https://openaccess.thecvf.com/content/CVPR2021/papers/Guo_Long-Tailed_Multi-Label_Visual_Recognition_by_Collaborative_Training_on_Uniform_and_CVPR_2021_paper.pdf","posterSession":"CVPR","authors":["Hao Guo","Song Wang"],"arXiv":null}},{"node":{"abstract":"We tackle the long-tailed visual recognition problem from the knowledge distillation perspective by proposing a Distill the Virtual Examples (DiVE) method. Specifically, by treating the predictions of a teacher model as virtual exam- ples, we prove that distilling from these virtual examples is equivalent to label distribution learning under certain con- straints. We show that when the virtual example distribu- tion becomes flatter than the original input distribution, the under-represented tail classes will receive significant im- provements, which is crucial in long-tailed recognition. The proposed DiVE method can explicitly tune the virtual exam- ple distribution to become flat. Extensive experiments on three benchmark datasets, including the large-scale iNat- uralist ones, justify that the proposed DiVE method can significantly outperform state-of-the-art methods. Further- more, additional analyses and experiments verify the virtual example interpretation, and demonstrate the effectiveness of tailored designs in DiVE for long-tailed problems.","citations":2,"title":"Distilling Virtual Examples for Long-Tailed Recognition","twitter":{"likes":5,"replies":0,"retweets":1,"ids":["1377879692741517315"]},"id":"eb516a8b-d646-59e5-8eb1-475a88c4cad2","s2id":"f61810f6b6202b2cac12236ecb0bfd1ecd230eb2","pdf":"https://openaccess.thecvf.com/content/ICCV2021/papers/He_Distilling_Virtual_Examples_for_Long-Tailed_Recognition_ICCV_2021_paper.pdf","posterSession":"ICCV","authors":["Yin-Yin He","Jianxin Wu","Xiu-Shen Wei"],"arXiv":"http://arxiv.org/abs/2103.15042"}},{"node":{"abstract":"The current evaluation protocol of long-tailed visual recognition trains the classification model on the long-tailed source label distribution and evaluates its performance on the uniform target label distribution. Such protocol has questionable practicality since the target may also be long-tailed. Therefore, we formulate long-tailed visual recognition as a label shift problem where the target and source label distributions are different. One of the significant hurdles in dealing with the label shift problem is the entanglement between the source label distribution and the model prediction. In this paper, we focus on disentangling the source label distribution from the model prediction. We first introduce a simple but overlooked baseline method that matches the target label distribution by post-processing the model prediction trained by the cross-entropy loss and the Softmax function. Although this method surpasses state-of-the-art methods on benchmark datasets, it can be further improved by directly disentangling the source label distribution from the model prediction in the training phase. Thus, we propose a novel method, LAbel distribution DisEntangling (LADE) loss based on the optimal bound of Donsker-Varadhan representation. LADE achieves state-of-the-art performance on benchmark datasets such as CIFAR-100-LT, Places-LT, ImageNet-LT, and iNaturalist 2018. Moreover, LADE outperforms existing methods on various shifted target label distributions, showing the general adaptability of our proposed method.","citations":5,"title":"Disentangling Label Distribution for Long-Tailed Visual Recognition","twitter":{"likes":100,"replies":0,"retweets":15,"ids":["1420693080844509187","1420776129242140674","1425446399383019524"]},"id":"2451b202-33ac-5249-98a2-0d536a193e03","s2id":"bf9386df7a517cb888f0229a1815b76e3826f150","pdf":"https://openaccess.thecvf.com/content/CVPR2021/papers/Hong_Disentangling_Label_Distribution_for_Long-Tailed_Visual_Recognition_CVPR_2021_paper.pdf","posterSession":"CVPR","authors":["Youngkyu Hong","Seungju Han","Kwanghee Choi","Seokjun Seo","Beomsu Kim","Buru Chang"],"arXiv":"http://arxiv.org/abs/2012.00321"}},{"node":{"abstract":"Real-world training data usually exhibits long-tailed distribution, where several majority classes have a significantly larger number of samples than the remaining minority classes. This imbalance degrades the performance of typical supervised learning algorithms designed for balanced training sets. In this paper, we address this issue by augmenting minority classes with a recently proposed implicit semantic data augmentation (ISDA) algorithm, which produces diversified augmented samples by translating deep features along many semantically meaningful directions. Importantly, given that ISDA estimates the class-conditional statistics to obtain semantic directions, we find it ineffective to do this on minority classes due to the insufficient training data. To this end, we propose a novel approach to learn transformed semantic directions with meta-learning automatically. In specific, the augmentation strategy during training is dynamically optimized, aiming to minimize the loss on a small balanced validation set, which is approximated via a meta update step. Extensive empirical results on CIFAR-LT-10/100, ImageNet-LT, and iNaturalist 2017/2018 validate the effectiveness of our method.","citations":4,"title":"MetaSAug: Meta Semantic Augmentation for Long-Tailed Visual Recognition","twitter":{"likes":96,"replies":1,"retweets":10,"ids":["1420693080659963905","1374562460359823376","1420776136355815428","1374875858653966338","1376297802763018240","1376297656834736130"]},"id":"5d2abd82-785a-5799-ba19-49bf2c8c6e18","s2id":"505422c6e07b356969e641cdb0985ab2c85ccae4","pdf":"https://openaccess.thecvf.com/content/CVPR2021/papers/Li_MetaSAug_Meta_Semantic_Augmentation_for_Long-Tailed_Visual_Recognition_CVPR_2021_paper.pdf","posterSession":"CVPR","authors":["Shuang Li","Kaixiong Gong","Chi Harold Liu","Yulin Wang","Feng Qiao","Xinjing Cheng"],"arXiv":"http://arxiv.org/abs/2103.12579"}},{"node":{"abstract":"Deep learning has achieved remarkable progress for visual recognition on large-scale balanced datasets but still performs poorly on real-world long-tailed data. Previous methods often adopt class re-balanced training strategies to effectively alleviate the imbalance issue, but might be a risk of over-fitting tail classes. The recent decoupling method overcomes over-fitting issues by using a multi-stage training scheme, yet, it is still incapable of capturing tail class information in the feature learning stage. In this paper, we show that soft label can serve as a powerful solution to incorporate label correlation into a multi-stage training scheme for long-tailed recognition. The intrinsic relation between classes embodied by soft labels turns out to be helpful for long-tailed recognition by transferring knowledge from head to tail classes.  Specifically, we propose a conceptually simple yet particularly effective multi-stage training scheme, termed as Self Supervised to Distillation (SSD). This scheme is composed of two parts. First, we introduce a self-distillation framework for long-tailed recognition, which can mine the label relation automatically. Second, we present a new distillation label generation module guided by self-supervision. The distilled labels integrate information from both label and data domains that can model long-tailed distribution effectively. We conduct extensive experiments and our method achieves the state-of-the-art results on three long-tailed recognition benchmarks: ImageNet-LT, CIFAR100-LT and iNaturalist 2018. Our SSD outperforms the strong LWS baseline by from 2.7% to 4.5% on various datasets.","citations":1,"title":"Self Supervision to Distillation for Long-Tailed Visual Recognition","twitter":{"likes":0,"replies":1,"retweets":0,"ids":["1436370753759465478","1436223836593299460"]},"id":"3de1b620-d248-5340-ae28-1a144543cfe3","s2id":"2eb3766268a24de4f59d42c8cdf757d8f3f0b91c","pdf":"https://openaccess.thecvf.com/content/ICCV2021/papers/Li_Self_Supervision_to_Distillation_for_Long-Tailed_Visual_Recognition_ICCV_2021_paper.pdf","posterSession":"ICCV","authors":["Tianhao Li","Limin Wang","Gangshan Wu"],"arXiv":"http://arxiv.org/abs/2109.04075"}},{"node":{"abstract":"The problem of long-tailed recognition, where the number of examples per class is highly unbalanced, is considered. It is hypothesized that the well known tendency of standard classifier training to overfit to popular classes can be exploited for effective transfer learning. Rather than eliminating this overfitting, e.g. by adopting popular class-balanced sampling methods, the learning algorithm should instead leverage this overfitting to transfer geometric information from popular to low-shot classes. A new classifier architecture, GistNet, is proposed to support this goal, using constellations of classifier parameters to encode the class geometry. A new learning algorithm is then proposed for GeometrIc Structure Transfer (GIST), with resort to a combination of loss functions that combine class-balanced and random sampling to guarantee that, while overfitting to the popular classes is restricted to geometric parameters, it is leveraged to transfer class geometry from popular to few-shot classes. This enables better generalization for few-shot classes without the need for the manual specification of class weights, or even the explicit grouping of classes into different types. Experiments on two popular long-tailed recognition datasets show that GistNet outperforms existing solutions to this problem.","citations":1,"title":"GistNet: A Geometric Structure Transfer Network for Long-Tailed Recognition","twitter":null,"id":"2010d916-6f70-57ab-85e2-675410fc0b77","s2id":"33d5cdbadc95d1ef093104bca9d29c2610fbb5fc","pdf":"https://openaccess.thecvf.com/content/ICCV2021/papers/Liu_GistNet_A_Geometric_Structure_Transfer_Network_for_Long-Tailed_Recognition_ICCV_2021_paper.pdf","posterSession":"ICCV","authors":["Bo Liu","Haoxiang Li","Hao Kang","Gang Hua","Nuno Vasconcelos"],"arXiv":"http://arxiv.org/abs/2105.00131"}},{"node":{"abstract":"Predicting the future states of dynamic traffic actors enables autonomous systems to avoid accidents and operate safely. Remarkably, the most critical scenarios are much less frequent and more complex than the uncritical ones. Therefore, uncritical cases dominate the prediction. In this paper, we address specifically the challenging scenarios at the long tail of the dataset distribution. Our analysis shows that the common losses tend to place challenging cases sub-optimally in the embedding space. As a consequence, we propose to supplement the usual loss with a loss that places challenging cases closer to each other in the embedding space. This triggers sharing information among challenging cases and learning specific predictive features. We show on four public datasets that this leads to improved performance on the hard scenarios while the overall performance stays stable. The approach is agnostic w.r.t. the used network architecture, input modality or viewpoint, and can be integrated into existing solutions easily.","citations":0,"title":"On Exposing the Challenging Long Tail in Future Prediction of Traffic Actors","twitter":{"likes":6,"replies":1,"retweets":1,"ids":["1419277604423716865","1451064769314656260"]},"id":"542fd236-ffaa-5ff9-8c6c-adcbb2dcc876","s2id":"6d93eee77a28b5448f9e6a826b200fd40a9f6a6a","pdf":"https://openaccess.thecvf.com/content/ICCV2021/papers/Makansi_On_Exposing_the_Challenging_Long_Tail_in_Future_Prediction_of_ICCV_2021_paper.pdf","posterSession":"ICCV","authors":["Osama Makansi","Özgün Çiçek","Yassine Marrakchi","Thomas Brox"],"arXiv":null}},{"node":{"abstract":"Real-world data is often unbalanced and long-tailed, but deep models struggle to recognize rare classes in the presence of frequent classes. To address unbalanced data, most studies try balancing the data, the loss, or the classifier to reduce classification bias towards head classes. Far less attention has been given to the latent representations learned with unbalanced data. We show that the feature extractor part of deep networks suffers greatly from this bias. We propose a new loss based on robustness theory, which encourages the model to learn high-quality representations for both head and tail classes. While the general form of the robustness loss may be hard to compute, we further derive an easy-to-compute upper bound that can be minimized efficiently. This procedure reduces representation bias towards head classes in the feature space and achieves new SOTA results on CIFAR100-LT, ImageNet-LT, and iNaturalist long-tail benchmarks. We find that training with robustness increases recognition accuracy of tail classes while largely maintaining the accuracy of head classes. The new robustness loss can be combined with various classifier balancing techniques and can be applied to representations at several layers of the deep model.","citations":2,"title":"Distributional Robustness Loss for Long-Tail Learning","twitter":null,"id":"750c23e4-9d98-5b0a-a5bf-4ca975593c8a","s2id":"6f9918ad83b2e6058acfc15ed66601879b855494","pdf":"https://openaccess.thecvf.com/content/ICCV2021/papers/Samuel_Distributional_Robustness_Loss_for_Long-Tail_Learning_ICCV_2021_paper.pdf","posterSession":"ICCV","authors":["Dvir Samuel","Gal Chechik"],"arXiv":"http://arxiv.org/abs/2104.03066"}},{"node":{"abstract":"Recently proposed decoupled training methods emerge as a dominant paradigm for long-tailed object detection. But they require an extra fine-tuning stage, and the disjointed optimization of representation and classifier might lead to suboptimal results. However, end-to-end training methods, like equalization loss (EQL), still perform worse than decoupled training methods. In this paper, we reveal the main issue in long-tailed object detection is the imbalanced gradients between positives and negatives, and find that EQL does not solve it well. To address the problem of imbalanced gradients, we introduce a new version of equalization loss, called equalization loss v2 (EQL v2), a novel gradient guided reweighing mechanism that re-balances the training process for each category independently and equally. Extensive experiments are performed on the challenging LVIS benchmark. EQL v2 outperforms origin EQL by about 4 points overall AP with 14 - 18 points improvements on the rare categories. More importantly, it also surpasses decoupled training methods. Without further tuning for the Open Images dataset, EQL v2 improves EQL by 7.3 points AP, showing strong generalization ability. Codes have been released at https://github.com/tztztztztz/eqlv2","citations":13,"title":"Equalization Loss v2: A New Gradient Balance Approach for Long-Tailed Object Detection","twitter":{"likes":76,"replies":0,"retweets":10,"ids":["1408009505179308033","1339815431679344640","1408100104788082688","1408100102674305027"]},"id":"b61f78f6-9652-5b4a-a69d-e96036b280de","s2id":"f21aa5f92699e621b2a4110cc00fd61f41469347","pdf":"https://openaccess.thecvf.com/content/CVPR2021/papers/Tan_Equalization_Loss_v2_A_New_Gradient_Balance_Approach_for_Long-Tailed_CVPR_2021_paper.pdf","posterSession":"CVPR","authors":["Jingru Tan","Xin Lu","Gang Zhang","Changqing Yin","Quanquan Li"],"arXiv":"http://arxiv.org/abs/2012.08548"}},{"node":{"abstract":"To address the problem of long-tail distribution for the large vocabulary object detection task, existing methods usually divide the whole categories into several groups and treat each group with different strategies. These methods bring the following two problems. One is the training inconsistency between adjacent categories of similar sizes, and the other is that the learned model is lack of discrimination for tail categories which are semantically similar to some of the head categories. In this paper, we devise a novel Adaptive Class Suppression Loss (ACSL) to effectively tackle the above problems and improve the detection performance of tail categories. Specifically, we introduce a statistic-free perspective to analyze the long-tail distribution, breaking the limitation of manual grouping. According to this perspective, our ACSL adjusts the suppression gradients for each sample of each class adaptively, ensuring the training consistency and boosting the discrimination for rare categories. Extensive experiments on long-tail datasets LVIS and Open Images show that the our ACSL achieves 5.18% and 5.2% improvements with ResNet50-FPN, and sets a new state of the art. Code and models are available at https://github.com/CASIA-IVA-Lab/ACSL.","citations":4,"title":"Adaptive Class Suppression Loss for Long-Tail Object Detection","twitter":{"likes":14,"replies":0,"retweets":10,"ids":["1379269809095892994"]},"id":"a2af57f0-e067-5249-9715-b5dbbb810316","s2id":"5c6407c5a357c187dd1bc1c03a83b3994d96e1e7","pdf":"https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_Adaptive_Class_Suppression_Loss_for_Long-Tail_Object_Detection_CVPR_2021_paper.pdf","posterSession":"CVPR","authors":["Tong Wang","Yousong Zhu","Chaoyang Zhao","Wei Zeng","Jinqiao Wang","Ming Tang"],"arXiv":"http://arxiv.org/abs/2104.00885"}},{"node":{"abstract":"Learning discriminative image representations plays a vital role in long-tailed image classification because it can ease the classifier learning in imbalanced cases. Given the promising performance contrastive learning has shown recently in representation learning, in this work, we explore effective supervised contrastive learning strategies and tailor them to learn better image representations from imbalanced data in order to boost the classification accuracy thereon. Specifically, we propose a novel hybrid network structure being composed of a supervised contrastive loss to learn image representations and a cross-entropy loss to learn classifiers, where the learning is progressively transited from feature learning to the classifier learning to embody the idea that better features make better classifiers. We explore two variants of contrastive loss for feature learning, which vary in the forms but share a common idea of pulling the samples from the same class together in the normalized embedding space and pushing the samples from different classes apart. One of them is the recently proposed supervised contrastive (SC) loss, which is designed on top of the state-of-the-art unsupervised contrastive loss by incorporating positive samples from the same class. The other is a prototypical supervised contrastive (PSC) learning strategy which addresses the intensive memory consumption in standard SC loss and thus shows more promise under limited memory budget. Extensive experiments on three long-tailed classification datasets demonstrate the advantage of the proposed contrastive learning based hybrid networks in long-tailed classification.","citations":4,"title":"Contrastive Learning Based Hybrid Networks for Long-Tailed Image Classification","twitter":{"likes":116,"replies":0,"retweets":13,"ids":["1417431592289685536","1417499285654958080"]},"id":"053f9350-45c6-53b4-8da9-3a90aec4e4b0","s2id":"f5fc6db18cfcd2f26fb700194bfdd07b0cfca2a5","pdf":"https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_Contrastive_Learning_Based_Hybrid_Networks_for_Long-Tailed_Image_Classification_CVPR_2021_paper.pdf","posterSession":"CVPR","authors":["Peng Wang","Kai Han","Xiu-Shen Wei","Lei Zhang","Lei Wang"],"arXiv":"http://arxiv.org/abs/2103.14267"}},{"node":{"abstract":"Instance segmentation has witnessed a remarkable progress on class-balanced benchmarks. However, they fail to perform as accurately in real-world scenarios, where the category distribution of objects naturally comes with a long tail. Instances of head classes dominate a long-tailed dataset and they serve as negative samples of tail categories. The overwhelming gradients of negative samples on tail classes lead to a biased learning process for classifiers. Consequently, objects of tail categories are more likely to be misclassified as backgrounds or head categories. To tackle this problem, we propose Seesaw Loss to dynamically re-balance gradients of positive and negative samples for each category, with two complementary factors, i.e., mitigation factor and compensation factor. The mitigation factor reduces punishments to tail categories w.r.t. the ratio of cumulative training instances between different categories. Meanwhile, the compensation factor increases the penalty of misclassified instances to avoid false positives of tail categories. We conduct extensive experiments on Seesaw Loss with mainstream frameworks and different data sampling strategies. With a simple end-to-end training pipeline, Seesaw Loss obtains significant gains over Cross-Entropy Loss, and achieves state-of-the-art performance on LVIS dataset without bells and whistles. Code is available at https://github.com/open-mmlab/mmdetection.","citations":11,"title":"Seesaw Loss for Long-Tailed Instance Segmentation","twitter":{"likes":54,"replies":2,"retweets":10,"ids":["1340312833490182146","1300357714657202178","1432765465391091717","1299607575814483968","1299356097569009665","1298088668486221824"]},"id":"23f7cf45-9471-5c3e-9b7e-1f27b923073a","s2id":"4644b8204ff7fcc3b0835ffad2f4216911ac9426","pdf":"https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_Seesaw_Loss_for_Long-Tailed_Instance_Segmentation_CVPR_2021_paper.pdf","posterSession":"CVPR","authors":["Jiaqi Wang","Wenwei Zhang","Yuhang Zang","Yuhang Cao","Jiangmiao Pang","Tao Gong","Kai Chen","Ziwei Liu","Chen Change Loy","Dahua Lin"],"arXiv":"http://arxiv.org/abs/2008.10032"}},{"node":{"abstract":"Instance segmentation is an active topic in computer vision that is usually solved by using supervised learning approaches over very large datasets composed of object level masks. Obtaining such a dataset for any new domain can be very expensive and time-consuming. In addition, models trained on certain annotated categories do not generalize well to unseen objects. The goal of this paper is to propose a method that can perform unsupervised discovery of long-tail categories in instance segmentation, through learning instance embeddings of masked regions. Leveraging rich relationship and hierarchical structure between objects in the images, we propose self-supervised losses for learning mask embeddings. Trained on COCO dataset without additional annotations of the long-tail objects, our model is able to discover novel and more fine-grained objects than the common categories in COCO. We show that the model achieves competitive quantitative results on LVIS as compared to the supervised and partially supervised methods.","citations":2,"title":"Unsupervised Discovery of the Long-Tail in Instance Segmentation Using Hierarchical Self-Supervision","twitter":{"likes":3,"replies":2,"retweets":0,"ids":["1381323294750633984","1391905990350721024","1380747981478113282"]},"id":"45c8c2d3-781d-50f4-a883-2f044ed2540d","s2id":"0c13154582fe70b6b46184033610528119a60a22","pdf":"https://openaccess.thecvf.com/content/CVPR2021/papers/Weng_Unsupervised_Discovery_of_the_Long-Tail_in_Instance_Segmentation_Using_Hierarchical_CVPR_2021_paper.pdf","posterSession":"CVPR","authors":["Zhenzhen Weng","Mehmet Giray Ogut","Shai Limonchik","Serena Yeung"],"arXiv":"http://arxiv.org/abs/2104.01257"}},{"node":{"abstract":"Adversarial robustness has attracted extensive studies recently by revealing the vulnerability and intrinsic characteristics of deep networks. However, existing works on adversarial robustness mainly focus on balanced datasets, while real-world data usually exhibits a long-tailed distribution. To push adversarial robustness towards more realistic scenarios, in this work we investigate the adversarial vulnerability as well as defense under long-tailed distributions. In particular, we first reveal the negative impacts induced by imbalanced data on both recognition performance and adversarial robustness, uncovering the intrinsic challenges of this problem. We then perform a systematic study on existing long-tailed recognition methods in conjunction with the adversarial training framework. Several valuable observations are obtained: 1) natural accuracy is relatively easy to improve, 2) fake gain of robust accuracy exists under unreliable evaluation, and 3) boundary error limits the promotion of robustness. Inspired by these observations, we propose a clean yet effective framework, RoBal, which consists of two dedicated modules, a scale-invariant classifier and data re-balancing via both margin engineering at training stage and boundary adjustment during inference. Extensive experiments demonstrate the superiority of our approach over other state-of-the-art defense methods. To our best knowledge, we are the first to tackle adversarial robustness under long-tailed distributions, which we believe would be a significant step towards real-world robustness. Our code is available at: https://github.com/wutong16/Adversarial_Long-Tail.","citations":6,"title":"Adversarial Robustness Under Long-Tailed Distribution","twitter":{"likes":36,"replies":1,"retweets":4,"ids":["1381447834604511233","1381447986266406912"]},"id":"60364c62-f977-5d15-abec-afe59abaf007","s2id":"31d5494ad29e93dc22a00965e7bbc21c77e7634f","pdf":"https://openaccess.thecvf.com/content/CVPR2021/papers/Wu_Adversarial_Robustness_Under_Long-Tailed_Distribution_CVPR_2021_paper.pdf","posterSession":"CVPR","authors":["Tong Wu","Ziwei Liu","Qingqiu Huang","Yu Wang","Dahua Lin"],"arXiv":"http://arxiv.org/abs/2104.02703"}},{"node":{"abstract":"Recent methods for long-tailed instance segmentation still struggle on rare object classes with few training data. We propose a simple yet effective method, Feature Augmentation and Sampling Adaptation (FASA), that addresses the data scarcity issue by augmenting the feature space especially for rare classes. Both the Feature Augmentation (FA) and feature sampling components are adaptive to the actual training status -- FA is informed by the feature mean and variance of observed real samples from past iterations, and we sample the generated virtual features in a loss-adapted manner to avoid over-fitting. FASA does not require any elaborate loss design, and removes the need for inter-class transfer learning that often involves large cost and manually-defined head/tail class groups. We show FASA is a fast, generic method that can be easily plugged into standard or long-tailed segmentation frameworks, with consistent performance gains and little added cost. FASA is also applicable to other tasks like long-tailed classification with state-of-the-art performance.","citations":3,"title":"FASA: Feature Augmentation and Sampling Adaptation for Long-Tailed Instance Segmentation","twitter":{"likes":0,"replies":0,"retweets":0,"ids":["1365746803421958151"]},"id":"a6ea3d0b-055e-5686-925b-129b9b26ec07","s2id":"240a14fab0f26a861d750287fcafdaf35d15b51c","pdf":"https://openaccess.thecvf.com/content/ICCV2021/papers/Zang_FASA_Feature_Augmentation_and_Sampling_Adaptation_for_Long-Tailed_Instance_Segmentation_ICCV_2021_paper.pdf","posterSession":"ICCV","authors":["Yuhang Zang","Chen Huang","Chen Change Loy"],"arXiv":"http://arxiv.org/abs/2102.12867"}},{"node":{"abstract":"Despite the success of the deep neural networks, it remains challenging to effectively build a system for long-tail visual recognition tasks. To address this problem, we first investigate the performance bottleneck of the two-stage learning framework via ablative study. Motivated by our discovery, we develop a unified distribution alignment strategy for long-tail visual recognition. Particularly, we first propose an adaptive calibration strategy for each data point to calibrate its classification scores. Then we introduce a generalized re-weight method to incorporate the class prior, which provides a flexible and unified solution to copy with diverse scenarios of various visual recognition tasks. We validate our method by extensive experiments on four tasks, including image classification, semantic segmentation, object detection, and instance segmentation. Our approach achieves the state-of-the-art results across all four recognition tasks with a simple and unified framework.","citations":11,"title":"Distribution Alignment: A Unified Framework for Long-Tail Visual Recognition","twitter":{"likes":8,"replies":0,"retweets":4,"ids":["1378997947317153794"]},"id":"1786ef32-78d9-5d66-8011-34586d200db6","s2id":"51a33b04933f932c3a1425339c4412be89a2bdb5","pdf":"https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_Distribution_Alignment_A_Unified_Framework_for_Long-Tail_Visual_Recognition_CVPR_2021_paper.pdf","posterSession":"CVPR","authors":["Songyang Zhang","Zeming Li","Shipeng Yan","Xuming He","Jian Sun"],"arXiv":"http://arxiv.org/abs/2103.16370"}},{"node":{"abstract":"Many objects do not appear frequently enough in complex scenes (e.g., certain handbags in living rooms) for training an accurate object detector, but are often found frequently by themselves (e.g., in product images). Yet, these object-centric images are not effectively leveraged for improving object detection in scene-centric images. In this paper, we propose Mosaic of Object-centric images as Scene-centric images (MosaicOS), a simple and novel framework that is surprisingly effective at tackling the challenges of long-tailed object detection. Keys to our approach are three-fold: (i) pseudo scene-centric image construction from object-centric images for mitigating domain differences, (ii) high-quality bounding box imputation using the object-centric images' class labels, and (iii) a multi-stage training procedure. On LVIS object detection (and instance segmentation), MosaicOS leads to a massive 60% (and 23%) relative improvement in average precision for rare object categories. We also show that our framework can be compatibly used with other existing approaches to achieve even further gains. Our pre-trained models are publicly available at https://github.com/czhang0528/MosaicOS/.","citations":null,"title":"MosaicOS: A Simple and Effective Use of Object-Centric Images for Long-Tailed Object Detection","twitter":{"likes":8,"replies":0,"retweets":2,"ids":["1448053519827681280","1452426409725644801","1452426408979050500"]},"id":"7b6971d3-7a6f-52e1-8ff3-012d1692787b","s2id":"","pdf":"https://openaccess.thecvf.com/content/ICCV2021/papers/Zhang_MosaicOS_A_Simple_and_Effective_Use_of_Object-Centric_Images_for_ICCV_2021_paper.pdf","posterSession":"ICCV","authors":["Cheng Zhang","Tai-Yu Pan","Yandong Li","Hexiang Hu","Dong Xuan","Soravit Changpinyo","Boqing Gong","Wei-Lun Chao"],"arXiv":"http://arxiv.org/abs/2102.08884"}},{"node":{"abstract":"Label distributions in real-world are oftentimes long-tailed and imbalanced, resulting in biased models towards dominant labels. While long-tailed recognition has been extensively studied for image classification tasks, limited effort has been made for video domain. In this paper, we introduce VideoLT, a large-scale long-tailed video recognition dataset, as a step toward real-world video recognition. VideoLT contains 256,218 untrimmed videos, annotated into 1,004 classes with a long-tailed distribution. Through extensive studies, we demonstrate that state-of-the-art methods used for long-tailed image recognition do not perform well in the video domain due to the additional temporal dimension in video data. This motivates us to propose FrameStack, a simple yet effective method for long-tailed video recognition task. In particular, FrameStack performs sampling at the frame-level in order to balance class distributions, and the sampling ratio is dynamically determined using knowledge derived from the network during training. Experimental results demonstrate that FrameStack can improve classification performance without sacrificing overall accuracy. Code and dataset are available at: https://github.com/17Skye17/VideoLT.","citations":1,"title":"VideoLT: Large-Scale Long-Tailed Video Recognition","twitter":{"likes":22,"replies":2,"retweets":6,"ids":["1390477970331025410","1390834982290034690","1447762783685120002","1418485075059122178","1390807031293091846"]},"id":"4375ac6a-fc8b-5b4c-b048-9c04b279418a","s2id":"6a3ada4790748eaac6507f0fb9a2568b3d174c72","pdf":"https://openaccess.thecvf.com/content/ICCV2021/papers/Zhang_VideoLT_Large-Scale_Long-Tailed_Video_Recognition_ICCV_2021_paper.pdf","posterSession":"ICCV","authors":["Xing Zhang","Zuxuan Wu","Zejia Weng","Huazhu Fu","Jingjing Chen","Yu-Gang Jiang","Larry S. Davis"],"arXiv":"http://arxiv.org/abs/2105.02668"}},{"node":{"abstract":"Deep neural networks may perform poorly when training datasets are heavily class-imbalanced. Recently, two-stage methods decouple representation learning and classifier learning to improve performance. But there is still the vital issue of miscalibration. To address it, we design two methods to improve calibration and performance in such scenarios. Motivated by the fact that predicted probability distributions of classes are highly related to the numbers of class instances, we propose label-aware smoothing to deal with different degrees of over-confidence for classes and improve classifier learning. For dataset bias between these two stages due to different samplers, we further propose shifted batch normalization in the decoupling framework. Our proposed methods set new records on multiple popular long-tailed recognition benchmark datasets, including CIFAR-10-LT, CIFAR-100-LT, ImageNet-LT, Places-LT, and iNaturalist 2018.","citations":4,"title":"Improving Calibration for Long-Tailed Recognition","twitter":{"likes":185,"replies":0,"retweets":32,"ids":["1412350611769659393","1378801773662494720","1412441216600338433"]},"id":"17f438bf-bba6-58e2-b7f2-b43ba92e7538","s2id":"09ce0a5073dde62e6cff2d5dfed1944e024e951e","pdf":"https://openaccess.thecvf.com/content/CVPR2021/papers/Zhong_Improving_Calibration_for_Long-Tailed_Recognition_CVPR_2021_paper.pdf","posterSession":"CVPR","authors":["Zhisheng Zhong","Jiequan Cui","Shu Liu","Jiaya Jia"],"arXiv":"http://arxiv.org/abs/2104.00466"}}]}},"pageContext":{}},"staticQueryHashes":[]}