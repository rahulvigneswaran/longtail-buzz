{"componentChunkName":"component---src-pages-index-tsx","path":"/","result":{"data":{"allPaperDataJson":{"edges":[{"node":{"abstract":"The visual relationship recognition (VRR) task aims at understanding the pairwise visual relationships between interacting objects in an image. These relationships typically have a long-tail distribution due to their compositional nature. This problem gets more severe when the vocabulary becomes large, rendering this task very challenging. This paper shows that modeling an effective message-passing flow through an attention mechanism can be critical to tackling the compositionality and long-tail challenges in VRR. The method, called RelTransformer, represents each im- age as a fully-connected scene graph and restructures the whole scene into the relation-triplet and global-scene contexts. It directly passes the message from each element in the relation-triplet and global-scene contexts to the target relation via self-attention. We also design a learnable memory to augment the long-tail relation representation learning. Through extensive experiments, we find that our model generalizes well on many VRR benchmarks. Our model outperforms the best-performing models on two large-scale long-tail VRR benchmarks, VG8K-LT (+2.0% overall acc) and GQA-LT (+26.0% overall acc), both having a highly skewed distribution towards the tail. It also achieves strong results on the VG200 relation detection task. Our code is available at https://github.com/Vision-CAIR/ RelTransformer.","citations":2,"title":"RelTransformer: A Transformer-Based Long-Tail Visual Relationship Recognition","twitter":{"likes":4,"replies":1,"retweets":1,"ids":["1504418916440944641","1509016360214433801","1386852861141602307"]},"id":"e4e0bbc1-8ad4-5eb0-8414-3cac72feb57b","s2id":"20e45053c133eff939f800821230ff589bbdc626","pdf":"/content/CVPR2022/papers/Chen_RelTransformer_A_Transformer-Based_Long-Tail_Visual_Relationship_Recognition_CVPR_2022_paper.pdf","posterSession":"CVPR","authors":["Jun Chen","Aniket Agarwal","Sherif Abdelkarim","Deyao Zhu","Mohamed Elhoseiny"],"arXiv":"http://arxiv.org/abs/2104.11934"}},{"node":{"abstract":"Long-tailed instance segmentation is a challenging task due to the extreme imbalance of training samples among classes. It causes severe biases of the head classes (with majority samples) against the tailed ones. This renders \"how to appropriately define and alleviate the bias\" one of the most important issues. Prior works mainly use label distribution or mean score information to indicate a coarse-grained bias. In this paper, we explore to excavate the confusion matrix, which carries the fine-grained misclassification details, to relieve the pairwise biases, generalizing the coarse one. To this end, we propose a novel Pairwise Class Balance (PCB) method, built upon a confusion matrix which is updated during training to accumulate the ongoing prediction preferences. PCB generates fightback soft labels for regularization during training. Besides, an iterative learning paradigm is developed to support a progressive and smooth regularization in such debiasing. PCB can be plugged and played to any existing methods as a complement. Experiments results on LVIS demonstrate that our method achieves state-of-the-art performance without bells and whistles. Superior results across various architectures show the generalization ability. The code and trained models are available at https://github.com/megvii-research/PCB.","citations":2,"title":"Relieving Long-Tailed Instance Segmentation via Pairwise Class Balance","twitter":{"likes":4,"replies":4,"retweets":0,"ids":["1481069354888613890","1481069354888613890","1511175678418616324","1480812697205841927","1480755219604537346","1511175678418616324","1480812697205841927","1480755219604537346"]},"id":"85c8c7fb-95ee-5e54-862c-cdcaa0a88f3a","s2id":"96187ebced993aa046d93bb99418a154cfbce38a","pdf":"/content/CVPR2022/papers/He_Relieving_Long-Tailed_Instance_Segmentation_via_Pairwise_Class_Balance_CVPR_2022_paper.pdf","posterSession":"CVPR","authors":["Yin-Yin He","Peizhen Zhang","Xiu-Shen Wei","Xiangyu Zhang","Jian Sun"],"arXiv":"http://arxiv.org/abs/2201.02784"}},{"node":{"abstract":"Despite the recent success of long-tailed object detection, almost all long-tailed object detectors are developed based on the two-stage paradigm. In practice, one-stage detectors are more prevalent in the industry because they have a simple and fast pipeline that is easy to deploy. However, in the long-tailed scenario, this line of work has not been explored so far. In this paper, we investigate whether one-stage detectors can perform well in this case. We discover the primary obstacle that prevents one-stage detectors from achieving excellent performance is: categories suffer from different degrees of positive-negative imbalance problems under the long-tailed data distribution. The conventional focal loss balances the training process with the same modulating factor for all categories, thus failing to handle the long-tailed problem. To address this issue, we propose the Equalized Focal Loss (EFL) that rebalances the loss contribution of positive and negative samples of different categories independently according to their imbalance degrees. Specifically, EFL adopts a category-relevant modulating factor which can be adjusted dynamically by the training status of different categories. Extensive experiments conducted on the challenging LVIS v1 benchmark demonstrate the effectiveness of our proposed method. With an end-to-end training pipeline, EFL achieves 29.2% in terms of overall AP and obtains significant performance improvements on rare categories, surpassing all existing state-of-the-art methods. The code is available at https://github.com/ModelTC/EOD.","citations":2,"title":"Equalized Focal Loss for Dense Long-Tailed Object Detection","twitter":{"likes":2,"replies":2,"retweets":0,"ids":["1482316521665929218","1482316521665929218","1480517699495239681","1480392436924829697","1480517699495239681","1480392436924829697"]},"id":"32225fa5-f856-5094-adad-c1770847dcaa","s2id":"141cc68bb16576383ef1cb402765c7cb185a1fce","pdf":"/content/CVPR2022/papers/Li_Equalized_Focal_Loss_for_Dense_Long-Tailed_Object_Detection_CVPR_2022_paper.pdf","posterSession":"CVPR","authors":["Bo Li","Yongqiang Yao","Jingru Tan","Gang Zhang","Fengwei Yu","Jianwei Lu","Ye Luo"],"arXiv":"http://arxiv.org/abs/2201.02593"}},{"node":{"abstract":"General object detectors are always evaluated on hand-designed datasets, e.g., MS COCO and Pascal VOC, which tend to maintain balanced data distribution over different classes. However, it goes against the practical applications in the real world which suffer from a heavy class imbalance problem, known as the long-tailed object detection. In this paper, we propose a novel method, named Adaptive Hierarchical Representation Learning (AHRL), from a metric learning perspective to address long-tailed object detection. We visualize each learned class representation in the feature space, and observe that some classes, especially under-represented scarce classes, are prone to cluster with analogous ones due to the lack of discriminative representation. Inspired by this, we propose to split the whole feature space into a hierarchical structure and eliminate the problem in a divide-and-conquer way. AHRL contains a two-stage training paradigm. First, we train a normal baseline model and construct the hierarchical structure under the unsupervised clustering method. Then, we design an AHR loss that consists of two optimization objectives. On the one hand, AHR loss retains the hierarchical structure and keeps representation clusters away from each other. On the other hand, AHR loss adopts adaptive margins according to specific class pairs in the same cluster to further optimize locally. We conduct extensive experiments on the challenging LVIS dataset and AHRL outperforms all the existing state-of-the-art(SOTA) methods, with 29.1% segmentation AP and 29.3% box AP on LVIS v0.5 and 27.6% segmentation AP and 28.7% box AP on LVIS v1.0 based on ResNet-101. We hope our simple yet effective approach will serve as a solid baseline to help stimulate future research in long-tailed object detection. Code will be released soon.","citations":null,"title":"Adaptive Hierarchical Representation Learning for Long-Tailed Object Detection","twitter":null,"id":"3285677c-7a07-502e-bf4f-2b4abfb43b6c","s2id":"","pdf":"/content/CVPR2022/papers/Li_Adaptive_Hierarchical_Representation_Learning_for_Long-Tailed_Object_Detection_CVPR_2022_paper.pdf","posterSession":"CVPR","authors":["Banghuai Li"],"arXiv":null}},{"node":{"abstract":"In the real open world, data tends to follow long-tailed class distributions, motivating the well-studied long-tailed recognition (LTR) problem. Naive training produces models that are biased toward common classes in terms of higher accuracy. The key to addressing LTR is to balance various aspects including data distribution, training losses, and gradients in learning. We explore an orthogonal direction,  weight balancing , motivated by the empirical observation that the naively trained classifier has \"artificially\" larger weights in norm for common classes (because there exists abundant data to train them, unlike the rare classes). We investigate three techniques to balance weights, L2-normalization, weight decay, and MaxNorm. We first point out that L2-normalization \"perfectly\" balances per-class weights to be unit norm, but such a hard constraint might prevent classes from learning better classifiers. In contrast, weight decay penalizes larger weights more heavily and so learns small balanced weights; the MaxNorm constraint encourages growing small weights within a norm ball but caps all the weights by the radius. Our extensive study shows that both help learn balanced weights and greatly improve the LTR accuracy. Surprisingly, weight decay, although underexplored in LTR, significantly improves over prior work. Therefore, we adopt a two-stage training paradigm and propose a simple approach to LTR: (1) learning features using the cross-entropy loss by tuning weight decay, and (2) learning classifiers using class-balanced loss by tuning weight decay and MaxNorm. Our approach achieves the state-of-the-art accuracy on five standard benchmarks, serving as a future baseline for long-tailed recognition.","citations":2,"title":"Long-Tailed Recognition via Weight Balancing","twitter":{"likes":166,"replies":4,"retweets":30,"ids":["1508636533884166148","1508636533884166148","1526487407738036224","1526487407738036224","1539012116862074881","1539012116862074881","1508639308722290689","1508639308722290689"]},"id":"3697c677-7631-5eb4-9bec-3e626ff908d7","s2id":"cc8e9f795f83c5107816bd500acb13c4e200198c","pdf":"/content/CVPR2022/papers/Alshammari_Long-Tailed_Recognition_via_Weight_Balancing_CVPR_2022_paper.pdf","posterSession":"CVPR","authors":["Shaden Alshammari","Yu-Xiong Wang","Deva Ramanan","Shu Kong"],"arXiv":"http://arxiv.org/abs/2203.14197"}},{"node":{"abstract":"Long-tailed data is still a big challenge for deep neural networks, even though they have achieved great success on balanced data. We observe that vanilla training on long-tailed data with cross-entropy loss makes the instance-rich head classes severely squeeze the spatial distribution of the tail classes, which leads to difficulty in classifying tail class samples. Furthermore, the original cross-entropy loss can only propagate gradient short-lively because the gradient in softmax form rapidly approaches zero as the logit difference increases. This phenomenon is called softmax saturation. It is unfavorable for training on balanced data, but can be utilized to adjust the validity of the samples in long-tailed data, thereby solving the distorted embedding space of long-tailed problems. To this end, this paper proposes the Gaussian clouded logit adjustment by Gaussian perturbation of different class logits with varied amplitude. We define the amplitude of perturbation as cloud size and set relatively large cloud sizes to tail classes. The large cloud size can reduce the softmax saturation and thereby making tail class samples more active as well as enlarging the embedding space. To alleviate the bias in a classifier, we therefore propose the class-based effective number sampling strategy with classifier re-training. Extensive experiments on benchmark datasets validate the superior performance of the proposed method. Source code is available at: https://github.com/Keke921/GCLLoss.","citations":2,"title":"Long-Tailed Visual Recognition via Gaussian Clouded Logit Adjustment","twitter":null,"id":"4cb7a126-0fda-5538-ab0c-2480b95240f5","s2id":"8e5fa110cdeea486d2d29ec838e21d268af8fc0c","pdf":"/content/CVPR2022/papers/Li_Long-Tailed_Visual_Recognition_via_Gaussian_Clouded_Logit_Adjustment_CVPR_2022_paper.pdf","posterSession":"CVPR","authors":["Mengke Li","Yiu-ming Cheung","Yang Lu"],"arXiv":null}},{"node":{"abstract":"Classification on long-tailed distributed data is a challenging problem, which suffers from serious class-imbalance and accordingly unpromising performance especially on tail classes. Recently, the ensembling based methods achieve the state-of-the-art performance and show great potential. However, there are two limitations for current methods. First, their predictions are not trustworthy for failure-sensitive applications. This is especially harmful for the tail classes where the wrong predictions is basically frequent. Second, they assign unified numbers of experts to all samples, which is redundant for easy samples with excessive computational cost. To address these issues, we propose a Trustworthy Long-tailed Classification (TLC) method to jointly conduct classification and uncertainty estimation to identify hard samples in a multi-expert framework. Our TLC obtains the evidence-based uncertainty (EvU) and evidence for each expert, and then combines these uncertainties and evidences under the Dempster-Shafer Evidence Theory (DST). Moreover, we propose a dynamic expert engagement to reduce the number of engaged experts for easy samples and achieve efficiency while maintaining promising performances. Finally, we conduct comprehensive experiments on the tasks of classification, tail detection, OOD detection and failure prediction. The experimental results show that the proposed TLC outperforms existing methods and is trustworthy with reliable uncertainty.","citations":2,"title":"Trustworthy Long-Tailed Classification","twitter":{"likes":0,"replies":0,"retweets":0,"ids":["1508271289307656192","1461292632618016768","1508271289307656192","1461292632618016768"]},"id":"9a6759f9-18fd-5157-beeb-fdf643f78726","s2id":"6bdd6bc4b4a8c3e5314f2ffcd2e1a08ae6674c05","pdf":"/content/CVPR2022/papers/Li_Trustworthy_Long-Tailed_Classification_CVPR_2022_paper.pdf","posterSession":"CVPR","authors":["Bolian Li","Zongbo Han","Haining Li","Huazhu Fu","Changqing Zhang"],"arXiv":"http://arxiv.org/abs/2111.09030"}},{"node":{"abstract":"The networks trained on the long-tailed dataset vary remarkably, despite the same training settings, which shows the great uncertainty in long-tailed learning. To alleviate the uncertainty, we propose a Nested Collaborative Learning (NCL), which tackles the problem by collaboratively learning multiple experts together. NCL consists of two core components, namely Nested Individual Learning (NIL) and Nested Balanced Online Distillation (NBOD), which focus on the individual supervised learning for each single expert and the knowledge transferring among multiple experts, respectively. To learn representations more thoroughly, both NIL and NBOD are formulated in a nested way, in which the learning is conducted on not just all categories from a full perspective but some hard categories from a partial perspective. Regarding the learning in the partial perspective, we specifically select the negative categories with high predicted scores as the hard categories by using a proposed Hard Category Mining (HCM). In the NCL, the learning from two perspectives is nested, highly related and complementary, and helps the network to capture not only global and robust features but also meticulous distinguishing ability. Moreover, self-supervision is further utilized for feature enhancement. Extensive experiments manifest the superiority of our method with outperforming the state-of-the-art whether by using a single model or an ensemble.","citations":2,"title":"Nested Collaborative Learning for Long-Tailed Visual Recognition","twitter":{"likes":3,"replies":1,"retweets":1,"ids":["1509155722335981569","1509015755136077824","1509010907610771457"]},"id":"9cae62a5-8a0a-587c-b053-148e61edff1f","s2id":"44bdf13111592afad8861a5ee19ec4cebfb82b6b","pdf":"/content/CVPR2022/papers/Li_Nested_Collaborative_Learning_for_Long-Tailed_Visual_Recognition_CVPR_2022_paper.pdf","posterSession":"CVPR","authors":["Jun Li","Zichang Tan","Jun Wan","Zhen Lei","Guodong Guo"],"arXiv":"http://arxiv.org/abs/2203.15359"}},{"node":{"abstract":"Real-world data often exhibits long tail distributions with heavy class imbalance, where the majority classes can dominate the training process and alter the decision boundaries of the minority classes. Recently, researchers have investigated the potential of supervised contrastive learning for long-tailed recognition, and demonstrated that it provides a strong performance gain. In this paper, we show that while supervised contrastive learning can help improve performance, past baselines suffer from poor uniformity brought in by imbalanced data distribution. This poor uniformity manifests in samples from the minority class having poor separability in the feature space. To address this problem, we propose targeted supervised contrastive learning (TSC), which improves the uniformity of the feature distribution on the hypersphere. TSC first generates a set of targets uniformly distributed on a hypersphere. It then makes the features of different classes converge to these distinct and uniformly distributed targets during training. This forces all classes, including minority classes, to maintain a uniform distribution in the feature space, improves class boundaries, and provides better generalization even in the presence of long-tail data. Experiments on multiple datasets show that TSC achieves state-of-the-art performance on long-tailed recognition tasks.","citations":2,"title":"Targeted Supervised Contrastive Learning for Long-Tailed Recognition","twitter":{"likes":0,"replies":0,"retweets":0,"ids":["1465592262121906176"]},"id":"549c4c67-18dd-57f2-84e2-aaa067bb3388","s2id":"2121c6910b5f187fdaecf65981ed76a6a668a559","pdf":"/content/CVPR2022/papers/Li_Targeted_Supervised_Contrastive_Learning_for_Long-Tailed_Recognition_CVPR_2022_paper.pdf","posterSession":"CVPR","authors":["Tianhong Li","Peng Cao","Yuan Yuan","Lijie Fan","Yuzhe Yang","Rogerio S. Feris","Piotr Indyk","Dina Katabi"],"arXiv":"http://arxiv.org/abs/2111.13998"}},{"node":{"abstract":"The problem of class imbalanced data is that the generalization performance of the classifier deteriorates due to the lack of data from minority classes. In this paper, we propose a novel minority over-sampling method to augment diversified minority samples by leveraging the rich context of the majority classes as background images. To diversify the minority samples, our key idea is to paste an image from a minority class onto rich-context images from a majority class, using them as background images. Our method is simple and can be easily combined with the existing long-tailed recognition methods. We empirically prove the effectiveness of the proposed oversampling method through extensive experiments and ablation studies. Without any architectural changes or complex algorithms, our method achieves state-of-the-art performance on various long-tailed classification benchmarks. Our code is made available at https://github.com/naver-ai/cmo.","citations":2,"title":"The Majority Can Help the Minority: Context-Rich Minority Oversampling for Long-Tailed Classification","twitter":{"likes":7,"replies":3,"retweets":3,"ids":["1473694415965532162","1499662542826008576","1466510407095525377"]},"id":"e6abb322-1b60-5763-b0e3-0d31d2009972","s2id":"b1fe28f40bfe5e74222b5e3ba4f6fcd994313e3b","pdf":"/content/CVPR2022/papers/Park_The_Majority_Can_Help_the_Minority_Context-Rich_Minority_Oversampling_for_CVPR_2022_paper.pdf","posterSession":"CVPR","authors":["Seulki Park","Youngkyu Hong","Byeongho Heo","Sangdoo Yun","Jin Young Choi"],"arXiv":"http://arxiv.org/abs/2112.00412"}},{"node":{"abstract":"We introduce Retrieval Augmented Classification (RAC), a generic approach to augmenting standard image classification pipelines with an explicit retrieval module. RAC consists of a standard base image encoder fused with a parallel retrieval branch that queries a non-parametric external memory of pre-encoded images and associated text snippets. We apply RAC to the problem of long-tail classification and demonstrate a significant improvement over previous state-of-the-art on Places365-LT and iNaturalist-2018 (14.5% and 6.7% respectively), despite using only the training datasets themselves as the external information source. We demonstrate that RAC's retrieval module, without prompting, learns a high level of accuracy on tail classes. This, in turn, frees the base encoder to focus on common classes, and improve its performance thereon. RAC represents an alternative approach to utilizing large, pretrained models without requiring fine-tuning, as well as a first step towards more effectively making use of external memory within common computer vision architectures.","citations":2,"title":"Retrieval Augmented Classification for Long-Tail Visual Recognition","twitter":{"likes":96,"replies":0,"retweets":13,"ids":["1496677091944845314","1497136293251203075","1496757663262453761"]},"id":"56df2c8b-8878-5cd8-943a-33cc0b52e5c9","s2id":"15115f67452f3305b69e6886cee98ac466d42cd5","pdf":"/content/CVPR2022/papers/Long_Retrieval_Augmented_Classification_for_Long-Tail_Visual_Recognition_CVPR_2022_paper.pdf","posterSession":"CVPR","authors":["Alexander Long","Wei Yin","Thalaiyasingam Ajanthan","Vu Nguyen","Pulak Purkait","Ravi Garg","Alan Blair","Chunhua Shen","Anton van den Hengel"],"arXiv":"http://arxiv.org/abs/2202.11233"}},{"node":{"abstract":"Long-tail object detection suffers from poor performance on tail categories. We reveal that the real culprit lies in the extremely imbalanced distribution of the classifier's weight norm. For conventional softmax cross-entropy loss, such imbalanced weight norm distribution yields ill conditioned decision boundary for categories which have small weight norms. To get rid of this situation, we choose to maximize the cosine similarity between the learned feature and the weight vector of target category rather than the inner-product of them. The decision boundary between any two categories is the angular bisector of their weight vectors. Whereas, the absolutely equal decision boundary is suboptimal because it reduces the model's sensitivity to various categories. Intuitively, categories with rich data diversity should occupy a larger area in the classification space while categories with limited data diversity should occupy a slightly small space. Hence, we devise a Category-Aware Angular Margin Loss (C2AM Loss) to introduce an adaptive angular margin between any two categories. Specifically, the margin between two categories is proportional to the ratio of their classifiers' weight norms. As a result, the decision boundary is slightly pushed towards the category which has a smaller weight norm. We conduct comprehensive experiments on LVIS dataset. C2AM Loss brings 4.9 5.2 AP improvements on different detectors and backbones compared with baseline.","citations":2,"title":"C2AM Loss: Chasing a Better Decision Boundary for Long-Tail Object Detection","twitter":null,"id":"b3c4608f-0006-5f3e-aea9-c338b59813f4","s2id":"6b593ea4284000631673cda863ec9cc24d5cbab5","pdf":"/content/CVPR2022/papers/Wang_C2AM_Loss_Chasing_a_Better_Decision_Boundary_for_Long-Tail_Object_CVPR_2022_paper.pdf","posterSession":"CVPR","authors":["Tong Wang","Yousong Zhu","Yingying Chen","Chaoyang Zhao","Bin Yu","Jinqiao Wang","Ming Tang"],"arXiv":null}},{"node":{"abstract":"In this work, we introduce a novel strategy for long-tail recognition that addresses the tail classes' few-shot problem via training-free knowledge transfer. Our objective is to transfer knowledge acquired from information-rich common classes to semantically similar, and yet data-hungry, rare classes in order to obtain stronger tail class representations. We leverage the fact that class prototypes and learned cosine classifiers provide two different, complementary representations of class cluster centres in feature space, and use an attention mechanism to select and recompose learned classifiers features from common classes to obtain higher quality rare class representations. Our knowledge transfer process is training free, reducing overfitting risks, and can afford continual extension of classifiers to new classes. Experiments show that our approach can achieve significant performance boosts on rare classes while maintaining robust common class performance, outperforming directly comparable state-of-the-art models.","citations":2,"title":"Long-Tail Recognition via Compositional Knowledge Transfer","twitter":null,"id":"229c925f-5d3d-5f13-bd21-8ae36cae6581","s2id":"3e72d6de13ad6ab62318d7d07df76c02b24d280f","pdf":"/content/CVPR2022/papers/Parisot_Long-Tail_Recognition_via_Compositional_Knowledge_Transfer_CVPR_2022_paper.pdf","posterSession":"CVPR","authors":["Sarah Parisot","Pedro M. Esperan√ßa","Steven McDonagh","Tamas J. Madarasz","Yongxin Yang","Zhenguo Li"],"arXiv":null}},{"node":{"abstract":"Real-world data typically follow a long-tailed distribution, where a few majority categories occupy most of the data while most minority categories contain a limited number of samples. Classification models minimizing cross-entropy struggle to represent and classify the tail classes. Although the problem of learning unbiased classifiers has been well studied, methods for representing imbalanced data are under-explored. In this paper, we focus on representation learning for imbalanced data. Recently, supervised contrastive learning has shown promising performance on balanced data recently. However, through our theoretical analysis, we find that for long-tailed data, it fails to form a regular simplex which is an ideal geometric configuration for representation learning. To correct the optimization behavior of SCL and further improve the performance of long-tailed visual recognition, we propose a novel loss for balanced contrastive learning (BCL). Compared with SCL, we have two improvements in BCL: class-averaging, which balances the gradient contribution of negative classes; class-complement, which allows all classes to appear in every mini-batch. The proposed balanced contrastive learning (BCL) method satisfies the condition of forming a regular simplex and assists the optimization of cross-entropy. Equipped with BCL, the proposed two-branch framework can obtain a stronger feature representation and achieve competitive performance on long-tailed benchmark datasets such as CIFAR-10-LT, CIFAR-100-LT, ImageNet-LT, and iNaturalist2018.","citations":null,"title":"Balanced Contrastive Learning for Long-Tailed Visual Recognition","twitter":null,"id":"d2be32b0-9cb2-51dc-98ba-f8585c814836","s2id":"","pdf":"/content/CVPR2022/papers/Zhu_Balanced_Contrastive_Learning_for_Long-Tailed_Visual_Recognition_CVPR_2022_paper.pdf","posterSession":"CVPR","authors":["Jianggang Zhu","Zheng Wang","Jingjing Chen","Yi-Ping Phoebe Chen","Yu-Gang Jiang"],"arXiv":null}}]}},"pageContext":{}},"staticQueryHashes":[]}